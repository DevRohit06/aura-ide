{
	"data": {
		"models": [
			{
				"id": "claude-opus-4-1",
				"name": "Anthropic: Claude Opus 4.1",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-1-20250805",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250805",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-opus-4-1-20250805",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250805",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-1@20250805",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"endpointConfigs": { "global": { "providerModelId": "claude-opus-4-1@20250805" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-opus-4-1@20250805",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-opus-4-1-20250805-v1:0",
								"version": "20250805",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-opus-4-1-20250805-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250805",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-opus-4.1",
								"pricing": [{ "threshold": 0, "input": 0.00001583, "output": 0.00007913 }],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-opus-4.1",
							"pricing": [{ "threshold": 0, "input": 0.00001583, "output": 0.00007913 }],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15.83, "completion": 79.13 }
					}
				],
				"maxOutput": 32000,
				"trainingDate": "2025-08-05T00:00:00.000Z",
				"description": "Our most capable model with the highest level of intelligence and capability. Supports extended thinking, multilingual capabilities, and vision processing. Moderately fast latency with 32,000 max output tokens. Training data cut-off: March 2025. API model name: claude-opus-4-1-20250805",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-opus-4",
				"name": "Anthropic: Claude Opus 4",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-20250514",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250514",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-opus-4-20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4@20250514",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"endpointConfigs": { "global": { "providerModelId": "claude-opus-4@20250514" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-opus-4@20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-opus-4-20250514-v1:0",
								"version": "20250514",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-opus-4-20250514-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15, "completion": 75, "cacheRead": 1.5, "cacheWrite": 18.75 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-opus-4",
								"pricing": [{ "threshold": 0, "input": 0.00001583, "output": 0.00007913 }],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-opus-4",
							"pricing": [{ "threshold": 0, "input": 0.00001583, "output": 0.00007913 }],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 15.83, "completion": 79.13 }
					}
				],
				"maxOutput": 32000,
				"trainingDate": "2025-05-14T00:00:00.000Z",
				"description": "Our previous flagship model with very high intelligence and capability. Supports extended thinking, multilingual capabilities, and vision processing. Moderately fast latency with 32,000 max output tokens. Training data cut-off: March 2025. API model name: claude-opus-4-20250514",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-sonnet-4",
				"name": "Anthropic: Claude Sonnet 4",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-sonnet-4-20250514",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250514",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									},
									{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-sonnet-4-20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								},
								{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{ "prompt": 6, "completion": 22.5, "threshold": 200000 }
						]
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-sonnet-4@20250514",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									},
									{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"endpointConfigs": { "global": { "providerModelId": "claude-sonnet-4@20250514" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-sonnet-4@20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								},
								{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{ "prompt": 6, "completion": 22.5, "threshold": 200000 }
						]
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-sonnet-4-20250514-v1:0",
								"version": "20250514",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									},
									{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-sonnet-4-20250514-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								},
								{ "threshold": 200000, "input": 0.000006, "output": 0.0000225 }
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{ "prompt": 6, "completion": 22.5, "threshold": 200000 }
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-sonnet-4",
								"pricing": [{ "threshold": 0, "input": 0.00000633, "output": 0.00002374 }],
								"contextLength": 1000000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-sonnet-4",
							"pricing": [{ "threshold": 0, "input": 0.00000633, "output": 0.00002374 }],
							"contextLength": 1000000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 6.33, "completion": 23.74 }
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-05-14T00:00:00.000Z",
				"description": "High-performance model with high intelligence and balanced performance. Supports extended thinking, multilingual capabilities, and vision processing. Fast latency with 64,000 max output tokens. API model name: claude-sonnet-4-20250514",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-3.7-sonnet",
				"name": "Anthropic: Claude 3.7 Sonnet",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "anthropic",
								"author": "anthropic",
								"providerModelId": "claude-3-7-sonnet-20250219",
								"version": "20250219",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-3-7-sonnet-20250219",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250219",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-3-7-sonnet@20250219",
								"version": "vertex-2023-10-16",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "global": {} }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-3-7-sonnet@20250219",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-7-sonnet-20250219-v1:0",
								"version": "20250219",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-7-sonnet-20250219-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250219",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.7-sonnet",
								"pricing": [{ "threshold": 0, "input": 0.000003165, "output": 0.00001583 }],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.7-sonnet",
							"pricing": [{ "threshold": 0, "input": 0.000003165, "output": 0.00001583 }],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3.165, "completion": 15.83 }
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-02-19T00:00:00.000Z",
				"description": "High-performance model with toggleable extended thinking for complex reasoning tasks. Combines high intelligence with the ability to think through problems step-by-step. Fast latency with 64,000 max output tokens. API model name: claude-3-7-sonnet-20250219",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "claude-3.5-sonnet-v2",
				"name": "Anthropic: Claude 3.5 Sonnet v2",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-3-5-sonnet-20241022",
								"provider": "anthropic",
								"author": "anthropic",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-3-5-sonnet-20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-3-5-sonnet-v2@20241022",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"crossRegion": false,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"global": { "providerModelId": "claude-3-5-sonnet-v2@20241022" }
								}
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-3-5-sonnet-v2@20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-5-sonnet-20241022-v2:0",
								"version": "20241022",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-5-sonnet-20241022-v2:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "20241022",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.5-sonnet",
								"pricing": [{ "threshold": 0, "input": 0.000003165, "output": 0.00001583 }],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.5-sonnet",
							"pricing": [{ "threshold": 0, "input": 0.000003165, "output": 0.00001583 }],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3.165, "completion": 15.83 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-10-22T00:00:00.000Z",
				"description": "Our previous intelligent model with high level of intelligence and capability. Fast latency with multilingual and vision capabilities, but no extended thinking. 8,192 max output tokens. Training data cut-off: April 2024. Upgraded version API: claude-3-5-sonnet-20241022, Previous version API: claude-3-5-sonnet-20240620",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "claude-3.5-haiku",
				"name": "Anthropic: Claude 3.5 Haiku",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "anthropic",
								"author": "anthropic",
								"providerModelId": "claude-3-5-haiku-20241022",
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "anthropic",
							"providerModelId": "claude-3-5-haiku-20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25, "write1h": 2 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-3-5-haiku@20241022",
								"crossRegion": false,
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "global": {} }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "claude-3-5-haiku@20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-5-haiku-20241022-v1:0",
								"version": "20241022",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "us-east-1": {} }
							},
							"userConfig": { "region": "us-east-1", "location": "us-east-1" },
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-5-haiku-20241022-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"cacheMultipliers": { "cachedInput": 0.1, "write5m": 1.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "20241022",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.5-haiku",
								"pricing": [{ "threshold": 0, "input": 8.44e-7, "output": 0.00000422 }],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.5-haiku",
							"pricing": [{ "threshold": 0, "input": 8.44e-7, "output": 0.00000422 }],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.844, "completion": 4.220000000000001 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-10-22T00:00:00.000Z",
				"description": "Our fastest model. Intelligence at blazing speeds. Multilingual and vision capabilities. 8,192 max output tokens. Training data cut-off: July 2024. API model name: claude-3-5-haiku-20241022",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "gpt-4o",
				"name": "OpenAI: GPT-4o",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000025,
										"output": 0.00001,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"rateLimits": { "rpm": 10000, "tpm": 30000000, "tpd": 15000000000 },
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-4o",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000025,
									"output": 0.00001,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.5, "completion": 10, "cacheRead": 1.25 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000025,
										"output": 0.00001,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": { "rpm": 300, "tpm": 50000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "gpt-4o",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000025,
									"output": 0.00001,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.5, "completion": 10, "cacheRead": 1.25 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4o",
								"pricing": [{ "threshold": 0, "input": 0.00000264, "output": 0.00001055 }],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4o",
							"pricing": [{ "threshold": 0, "input": 0.00000264, "output": 0.00001055 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.64, "completion": 10.55 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-05-13T00:00:00.000Z",
				"description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4o-mini",
				"name": "OpenAI: GPT-4o-mini",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 6e-7,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"rateLimits": { "rpm": 30000, "tpm": 150000000, "tpd": 15000000000 },
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-4o-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 6e-7,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.15, "completion": 0.6, "cacheRead": 0.075 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 6e-7,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": { "rpm": 2000, "tpm": 200000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "gpt-4o-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 6e-7,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.15, "completion": 0.6, "cacheRead": 0.075 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4o-mini",
								"pricing": [{ "threshold": 0, "input": 1.6e-7, "output": 6.3e-7 }],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4o-mini",
							"pricing": [{ "threshold": 0, "input": 1.6e-7, "output": 6.3e-7 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.16, "completion": 0.63 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-07-18T00:00:00.000Z",
				"description": "GPT-4o mini is OpenAI's newest model after GPT-4 Omni, supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than GPT-3.5 Turbo. It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences common leaderboards.\n\nCheck out the launch announcement to learn more.\n\n#multimodal",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "chatgpt-4o-latest",
				"name": "OpenAI: ChatGPT-4o",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/chatgpt-4o-latest",
								"pricing": [{ "threshold": 0, "input": 0.00000528, "output": 0.00001582 }],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/chatgpt-4o-latest",
							"pricing": [{ "threshold": 0, "input": 0.00000528, "output": 0.00001582 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 5.28, "completion": 15.82 }
					},
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "chatgpt-4o-latest",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000005,
										"output": 0.00002,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"rateLimits": { "rpm": 10000, "tpm": 30000000, "tpd": 15000000000 },
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "chatgpt-4o-latest",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000005,
									"output": 0.00002,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 5, "completion": 20, "cacheRead": 2.5 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-08-14T00:00:00.000Z",
				"description": "OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of GPT-4o in that it has additional RLHF. It is intended for research and evaluation.\n\nOpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "o3",
				"name": "OpenAI: o3",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-2025-04-16",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"rateLimits": { "rpm": 10000, "tpm": 30000000, "tpd": 5000000000 },
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "o3-2025-04-16",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2, "completion": 8, "cacheRead": 0.5 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3",
								"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000844 }],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/o3",
							"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000844 }],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.1100000000000003, "completion": 8.440000000000001 }
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. o3 is succeeded by GPT-5.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o3-pro",
				"name": "OpenAI: o3 Pro",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-pro-2025-06-10",
								"provider": "openai",
								"author": "openai",
								"pricing": [{ "threshold": 0, "input": 0.00002, "output": 0.00008 }],
								"rateLimits": { "rpm": 10000, "tpm": 30000000, "tpd": 5000000000 },
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "o3-pro-2025-06-10",
							"pricing": [{ "threshold": 0, "input": 0.00002, "output": 0.00008 }],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 20, "completion": 80 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3-pro",
								"pricing": [{ "threshold": 0, "input": 0.0000211, "output": 0.0000844 }],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/o3-pro",
							"pricing": [{ "threshold": 0, "input": 0.0000211, "output": 0.0000844 }],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 21.1, "completion": 84.4 }
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers. o3-pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests. Since o3-pro is designed to tackle tough problems, some requests may take several minutes to finish.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o3-mini",
				"name": "OpenAI: o3 Mini",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"rateLimits": { "rpm": 30000, "tpm": 150000000, "tpd": 15000000000 },
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "o3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.1, "completion": 4.4, "cacheRead": 0.55 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"rateLimits": { "rpm": 20, "tpm": 200000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "o3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.1, "completion": 4.4, "cacheRead": 0.55 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3-mini",
								"pricing": [{ "threshold": 0, "input": 0.00000116, "output": 0.00000464 }],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/o3-mini",
							"pricing": [{ "threshold": 0, "input": 0.00000116, "output": 0.00000464 }],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.16, "completion": 4.64 }
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2023-10-01T00:00:00.000Z",
				"description": "o3-mini is our newest small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini. o3-mini supports key developer features, like Structured Outputs, function calling, and Batch API. This model supports the `reasoning_effort` parameter, which can be set to 'high', 'medium', or 'low' to control the thinking time of the model.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o4-mini",
				"name": "OpenAI: o4 Mini",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o4-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"rateLimits": { "rpm": 30000, "tpm": 150000000, "tpd": 15000000000 },
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "o4-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.1, "completion": 4.4, "cacheRead": 0.275 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o4-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"rateLimits": { "rpm": 20, "tpm": 20000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "o4-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.1, "completion": 4.4, "cacheRead": 0.275 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o4-mini",
								"pricing": [{ "threshold": 0, "input": 0.00000116, "output": 0.00000464 }],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/o4-mini",
							"pricing": [{ "threshold": 0, "input": 0.00000116, "output": 0.00000464 }],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.16, "completion": 4.64 }
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "o4-mini is our latest small o-series model. It's optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It's succeeded by GPT-5 mini.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "gpt-4.1",
				"name": "OpenAI: GPT-4.1",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 10000, "tpm": 30000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2, "completion": 8, "cacheRead": 0.5 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 50, "tpm": 50000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "gpt-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2, "completion": 8, "cacheRead": 0.5 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1",
								"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000844 }],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1",
							"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000844 }],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.1100000000000003, "completion": 8.440000000000001 }
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:23:05.000Z",
				"description": "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4.1-mini",
				"name": "OpenAI: GPT-4.1 Mini",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.5e-7,
										"output": 0.000001,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 30000, "tpm": 150000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-4.1-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.5e-7,
									"output": 0.000001,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.25, "completion": 1, "cacheRead": 0.0625 }
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 4e-7,
										"output": 0.0000016,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 200, "tpm": 200000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "gpt-4.1-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 4e-7,
									"output": 0.0000016,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.39999999999999997,
							"completion": 1.5999999999999999,
							"cacheRead": 0.09999999999999999
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1-mini",
								"pricing": [{ "threshold": 0, "input": 4.2e-7, "output": 0.00000169 }],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1-mini",
							"pricing": [{ "threshold": 0, "input": 4.2e-7, "output": 0.00000169 }],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.42, "completion": 1.69 }
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:23:01.000Z",
				"description": "GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider's polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4.1-nano",
				"name": "OpenAI: GPT-4.1 Nano",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-nano",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 30000, "tpm": 150000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-4.1-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"cacheRead": 0.024999999999999998
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-nano",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"cacheMultipliers": { "cachedInput": 0.3 }
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": { "rpm": 200, "tpm": 200000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "azure",
							"providerModelId": "gpt-4.1-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"cacheMultipliers": { "cachedInput": 0.3 }
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"cacheRead": 0.03
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1-nano",
								"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 4.2e-7 }],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1-nano",
							"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 4.2e-7 }],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.11, "completion": 0.42 }
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:22:49.000Z",
				"description": "For tasks that demand low latency, GPT-4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding  even higher than GPT-4o mini. It's ideal for tasks like classification or autocompletion.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-5",
				"name": "OpenAI: GPT-5",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"cacheMultipliers": { "cachedInput": 0.1 }
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": { "rpm": 15000, "tpm": 40000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-5",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"cacheMultipliers": { "cachedInput": 0.1 }
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.25, "completion": 10, "cacheRead": 0.12500000000000003 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5",
								"pricing": [{ "threshold": 0, "input": 0.00000132, "output": 0.00001055 }],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5",
							"pricing": [{ "threshold": 0, "input": 0.00000132, "output": 0.00001055 }],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.32, "completion": 10.55 }
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 is OpenAI's most advanced language model, featuring enhanced reasoning capabilities with 80% fewer factual errors than o3. It supports a 400K total context (272K input + 128K output), advanced tool calling with reliable chaining of dozens of calls, and a new verbosity parameter for response length control. Ideal for complex reasoning, multi-step planning, and applications requiring high accuracy.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-mini",
				"name": "OpenAI: GPT-5 Mini",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.5e-7,
										"output": 0.000002,
										"cacheMultipliers": { "cachedInput": 0.1 }
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": { "rpm": 30000, "tpm": 180000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-5-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.5e-7,
									"output": 0.000002,
									"cacheMultipliers": { "cachedInput": 0.1 }
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.25, "completion": 2, "cacheRead": 0.024999999999999998 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-mini",
								"pricing": [{ "threshold": 0, "input": 2.6e-7, "output": 0.00000211 }],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-mini",
							"pricing": [{ "threshold": 0, "input": 2.6e-7, "output": 0.00000211 }],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.26, "completion": 2.1100000000000003 }
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 Mini delivers GPT-5-level performance at a fraction of the cost and latency. With the same 400K context window and advanced capabilities including tool calling and verbosity control, it's optimized for speed and efficiency while maintaining strong reasoning and instruction-following capabilities. Perfect for high-volume applications requiring advanced AI capabilities with resource constraints.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-nano",
				"name": "OpenAI: GPT-5 Nano",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-nano",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 4e-7,
										"cacheMultipliers": { "cachedInput": 0.1 }
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": { "rpm": 30000, "tpm": 180000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-5-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 4e-7,
									"cacheMultipliers": { "cachedInput": 0.1 }
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.39999999999999997,
							"cacheRead": 0.005
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-nano",
								"pricing": [{ "threshold": 0, "input": 5e-8, "output": 4.2e-7 }],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-nano",
							"pricing": [{ "threshold": 0, "input": 5e-8, "output": 4.2e-7 }],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.049999999999999996, "completion": 0.42 }
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 Nano is the smallest and fastest model in the GPT-5 family, designed for ultra-low latency applications. Despite its compact size, it maintains the full 400K context window and delivers impressive performance on classification, completion, and simple reasoning tasks. Ideal for real-time applications, edge deployments, and high-throughput scenarios.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-chat-latest",
				"name": "OpenAI: GPT-5 Chat Latest",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-chat-latest",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"cacheMultipliers": { "cachedInput": 0.1 }
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": { "rpm": 15000, "tpm": 40000000, "tpd": 15000000000 },
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openai",
							"providerModelId": "gpt-5-chat-latest",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"cacheMultipliers": { "cachedInput": 0.1 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.25, "completion": 10, "cacheRead": 0.12500000000000003 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-chat",
								"pricing": [{ "threshold": 0, "input": 0.00000132, "output": 0.00001055 }],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-chat",
							"pricing": [{ "threshold": 0, "input": 0.00000132, "output": 0.00001055 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.32, "completion": 10.55 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-09-30T00:00:00.000Z",
				"description": "GPT-5 Chat Latest is a continuously updated version of GPT-5 optimized for conversational interactions. It receives regular updates with the latest improvements in dialogue management, safety, and helpfulness. Features a 128K context window and 16K max output tokens, making it ideal for focused conversations. Knowledge cutoff: September 30, 2024.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-oss-120b",
				"name": "OpenAI: gpt-oss-120b",
				"author": "openai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-120b",
								"provider": "deepinfra",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 4e-8,
										"output": 1.6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [
								{
									"threshold": 0,
									"input": 4e-8,
									"output": 1.6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": false,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 0.04, "completion": 0.16 }
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-120b",
								"provider": "groq",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 7.5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_completion_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 7.5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_completion_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.15, "completion": 0.75 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-oss-120b",
								"pricing": [{ "threshold": 0, "input": 3.7e-7, "output": 7.9e-7 }],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [{ "threshold": 0, "input": 3.7e-7, "output": 7.9e-7 }],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.37, "completion": 0.7899999999999999 }
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "gpt-oss-120b is our most powerful open-weight model, which fits into a single H100 GPU (117B parameters with 5.1B active parameters). Features permissive Apache 2.0 license, configurable reasoning effort (low, medium, high), full chain-of-thought access, fine-tunable parameters, and agentic capabilities including function calling, web browsing, Python code execution, and structured outputs.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_completion_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"max_tokens"
				]
			},
			{
				"id": "gpt-oss-20b",
				"name": "OpenAI: gpt-oss-20b",
				"author": "openai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-20b",
								"provider": "groq",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_completion_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "openai/gpt-oss-20b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_completion_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.09999999999999999, "completion": 0.5 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-oss-20b",
								"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 5.3e-7 }],
								"contextLength": 131000,
								"maxCompletionTokens": 131000,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "openai/gpt-oss-20b",
							"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 5.3e-7 }],
							"contextLength": 131000,
							"maxCompletionTokens": 131000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.11, "completion": 0.53 }
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "gpt-oss-20b is our medium-sized open-weight model for low latency, local, or specialized use-cases (21B parameters with 3.6B active parameters). Features permissive Apache 2.0 license, configurable reasoning effort (low, medium, high), full chain-of-thought access, fine-tunable parameters, and agentic capabilities including function calling, web browsing, Python code execution, and structured outputs.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_completion_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"max_tokens"
				]
			},
			{
				"id": "gemini-2.5-pro",
				"name": "Google: Gemini 2.5 Pro",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-pro",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"image": 0.00516,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.0000045
									},
									{ "threshold": 200000, "input": 0.0000025, "output": 0.000015, "image": 0.00516 }
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": { "rpm": 2000, "tpm": 8000000 },
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"image": 0.00516,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.0000045
								},
								{ "threshold": 200000, "input": 0.0000025, "output": 0.000015, "image": 0.00516 }
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"image": 5160,
							"cacheRead": 0.3125,
							"cacheWrite": 1.25
						},
						"pricingTiers": [
							{
								"prompt": 1.25,
								"completion": 10,
								"image": 5160,
								"cacheRead": 0.3125,
								"cacheWrite": 1.25,
								"threshold": 0
							},
							{ "prompt": 2.5, "completion": 15, "image": 5160, "threshold": 200000 }
						]
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-pro",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"image": 0.00516,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.0000045
									},
									{ "threshold": 200000, "input": 0.0000025, "output": 0.000015, "image": 0.00516 }
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "global": { "providerModelId": "gemini-2.5-pro" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "gemini-2.5-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"image": 0.00516,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.0000045
								},
								{ "threshold": 200000, "input": 0.0000025, "output": 0.000015, "image": 0.00516 }
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"image": 5160,
							"cacheRead": 0.3125,
							"cacheWrite": 1.25
						},
						"pricingTiers": [
							{
								"prompt": 1.25,
								"completion": 10,
								"image": 5160,
								"cacheRead": 0.3125,
								"cacheWrite": 1.25,
								"threshold": 0
							},
							{ "prompt": 2.5, "completion": 15, "image": 5160, "threshold": 200000 }
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-pro",
								"pricing": [{ "threshold": 0, "input": 0.00000264, "output": 0.00001582 }],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-pro",
							"pricing": [{ "threshold": 0, "input": 0.00000264, "output": 0.00001582 }],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.64, "completion": 15.82 }
					}
				],
				"maxOutput": 65536,
				"trainingDate": "2025-06-17T07:12:24",
				"description": "Gemini 2.5 Pro is Googles state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs thinking capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemini-2.5-flash",
				"name": "Google: Gemini 2.5 Flash",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000025,
										"image": 0.001238,
										"audio": 0.000001,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": { "rpm": 10000, "tpm": 8000000 },
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-flash",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000025,
									"image": 0.001238,
									"audio": 0.000001,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 0.3,
							"completion": 2.5,
							"audio": 1,
							"image": 1238,
							"cacheRead": 0.075,
							"cacheWrite": 0.3
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000025,
										"image": 0.001238,
										"audio": 0.000001,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "global": { "providerModelId": "gemini-2.5-flash" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "gemini-2.5-flash",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000025,
									"image": 0.001238,
									"audio": 0.000001,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 0.3,
							"completion": 2.5,
							"audio": 1,
							"image": 1238,
							"cacheRead": 0.075,
							"cacheWrite": 0.3
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-flash",
								"pricing": [{ "threshold": 0, "input": 3.2e-7, "output": 0.00000264 }],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-flash",
							"pricing": [{ "threshold": 0, "input": 3.2e-7, "output": 0.00000264 }],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.32, "completion": 2.64 }
					}
				],
				"maxOutput": 65535,
				"trainingDate": "2025-06-17T08:01:28",
				"description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, allowing fine-tuned control over its reasoning process.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemini-2.5-flash-lite",
				"name": "Google: Gemini 2.5 Flash Lite",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash-lite",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"audio": 3e-7,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": { "rpm": 30000, "tpm": 30000000 },
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-flash-lite",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"audio": 3e-7,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"audio": 0.3,
							"cacheRead": 0.024999999999999998,
							"cacheWrite": 0.09999999999999999
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash-lite",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"audio": 3e-7,
										"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "global": { "providerModelId": "gemini-2.5-flash-lite" } }
							},
							"userConfig": { "region": "global", "location": "global" },
							"provider": "vertex",
							"providerModelId": "gemini-2.5-flash-lite",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"audio": 3e-7,
									"cacheMultipliers": { "cachedInput": 0.25, "write5m": 1 },
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": false,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": false,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"audio": 0.3,
							"cacheRead": 0.024999999999999998,
							"cacheWrite": 0.09999999999999999
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-flash-lite",
								"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 4.2e-7 }],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-flash-lite",
							"pricing": [{ "threshold": 0, "input": 1.1e-7, "output": 4.2e-7 }],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.11, "completion": 0.42 }
					}
				],
				"maxOutput": 65535,
				"trainingDate": "2025-07-22T09:04:36",
				"description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemma2-9b-it",
				"name": "Google: Gemma 2",
				"author": "google",
				"contextLength": 8192,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemma2-9b-it",
								"provider": "groq",
								"author": "google",
								"pricing": [{ "threshold": 0, "input": 2e-7, "output": 2e-7, "image": 0 }],
								"contextLength": 8192,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "gemma2-9b-it",
							"pricing": [{ "threshold": 0, "input": 2e-7, "output": 2e-7, "image": 0 }],
							"contextLength": 8192,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.19999999999999998, "completion": 0.19999999999999998 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemma-2-9b-it",
								"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 2.1e-7 }],
								"contextLength": 8192,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "google/gemma-2-9b-it",
							"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 2.1e-7 }],
							"contextLength": 8192,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.21, "completion": 0.21 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-06-25T00:00:00.000Z",
				"description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"seed",
					"stop",
					"temperature",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "gemma-3-12b-it",
				"name": "Google: Gemma 3 12B",
				"author": "google",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "google/gemma-3-12b-it",
								"provider": "deepinfra",
								"author": "google",
								"pricing": [{ "threshold": 0, "input": 5e-8, "output": 1e-7 }],
								"rateLimits": { "rpm": 12000, "tpm": 60000000, "tpd": 6000000000 },
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "google/gemma-3-12b-it",
							"pricing": [{ "threshold": 0, "input": 5e-8, "output": 1e-7 }],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": false,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 0.049999999999999996, "completion": 0.09999999999999999 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-12-01T00:00:00.000Z",
				"description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "grok-code-fast-1",
				"name": "xAI: Grok Code Fast 1",
				"author": "xai",
				"contextLength": 256000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-code-fast-1",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 0.0000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": { "cachedInput": 0.1 }
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 10000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "xai",
							"providerModelId": "grok-code-fast-1",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 0.0000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": { "cachedInput": 0.1 }
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 10000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.19999999999999998, "completion": 1.5, "cacheRead": 0.02 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-code-fast-1",
								"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 0.00000158 }],
								"contextLength": 256000,
								"maxCompletionTokens": 10000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-code-fast-1",
							"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 0.00000158 }],
							"contextLength": 256000,
							"maxCompletionTokens": 10000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.21, "completion": 1.5799999999999998 }
					}
				],
				"maxOutput": 10000,
				"trainingDate": "2024-08-25T00:00:00.000Z",
				"description": "Speedy and economical reasoning model that excels at agentic coding. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-4",
				"name": "xAI: Grok 4",
				"author": "xai",
				"contextLength": 256000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-4",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": { "cachedInput": 0.25 }
									},
									{
										"threshold": 128000,
										"input": 0.000006,
										"output": 0.00003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": { "cachedInput": 0.125 }
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 256000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "xai",
							"providerModelId": "grok-4",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": { "cachedInput": 0.25 }
								},
								{
									"threshold": 128000,
									"input": 0.000006,
									"output": 0.00003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": { "cachedInput": 0.125 }
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 256000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3, "completion": 15, "web_search": 25000, "cacheRead": 0.75 },
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"web_search": 25000,
								"cacheRead": 0.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 30,
								"web_search": 25000,
								"cacheRead": 0.75,
								"threshold": 128000
							}
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-4",
								"pricing": [{ "threshold": 0, "input": 0.00000633, "output": 0.00003165 }],
								"contextLength": 256000,
								"maxCompletionTokens": 256000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-4",
							"pricing": [{ "threshold": 0, "input": 0.00000633, "output": 0.00003165 }],
							"contextLength": 256000,
							"maxCompletionTokens": 256000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 6.33, "completion": 31.65 }
					}
				],
				"maxOutput": 256000,
				"trainingDate": "2024-07-09T00:00:00.000Z",
				"description": "Latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-3",
				"name": "xAI: Grok 3",
				"author": "xai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-3",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "xai",
							"providerModelId": "grok-3",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3, "completion": 15, "web_search": 25000, "cacheRead": 0.75 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-3",
								"pricing": [{ "threshold": 0, "input": 0.00000528, "output": 0.00002638 }],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-3",
							"pricing": [{ "threshold": 0, "input": 0.00000528, "output": 0.00002638 }],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 5.28, "completion": 26.380000000000003 }
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "Excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-3-mini",
				"name": "xAI: Grok 3 Mini",
				"author": "xai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-3-mini",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": { "cachedInput": 0.25 }
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "xai",
							"providerModelId": "grok-3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": { "cachedInput": 0.25 }
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.3, "completion": 0.5, "web_search": 25000, "cacheRead": 0.075 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-3-mini",
								"pricing": [{ "threshold": 0, "input": 6.33e-7, "output": 0.00000422 }],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-3-mini",
							"pricing": [{ "threshold": 0, "input": 6.33e-7, "output": 0.00000422 }],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.633, "completion": 4.220000000000001 }
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "Lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "llama-4-scout",
				"name": "Llama-4-Scout-17B-16E",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.1e-7,
										"output": 3.4e-7,
										"request": 0,
										"image": 0.00036762,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.1e-7,
									"output": 3.4e-7,
									"request": 0,
									"image": 0.00036762,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.11, "completion": 0.33999999999999997, "image": 367.62 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-4-scout",
								"pricing": [{ "threshold": 0, "input": 6.9e-7, "output": 9e-7 }],
								"contextLength": 1048576,
								"maxCompletionTokens": 1048576,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-4-scout",
							"pricing": [{ "threshold": 0, "input": 6.9e-7, "output": 9e-7 }],
							"contextLength": 1048576,
							"maxCompletionTokens": 1048576,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.69, "completion": 0.8999999999999999 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Llama 4 instruction-tuned MoE (17B, 16 experts) for fast, high-quality chat, tool use, and multilingual reasoning with balanced latency and cost.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p"
				]
			},
			{
				"id": "llama-4-maverick",
				"name": "Llama-4-Maverick-17B-128E",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.19999999999999998, "completion": 0.6 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-4-maverick",
								"pricing": [{ "threshold": 0, "input": 6.6e-7, "output": 0.0000019 }],
								"contextLength": 1048576,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-4-maverick",
							"pricing": [{ "threshold": 0, "input": 6.6e-7, "output": 0.0000019 }],
							"contextLength": 1048576,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.66, "completion": 1.9 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Llama 4 instruction-tuned MoE (17B, 128 experts) targeting tougher reasoning and long-form tasks, trading more compute for higher response diversity and robustness.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p"
				]
			},
			{
				"id": "llama-guard-4",
				"name": "Llama-Guard-4-12B",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-guard-4-12b",
								"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 2.1e-7 }],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-guard-4-12b",
							"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 2.1e-7 }],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.21, "completion": 0.21 }
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-Guard-4-12B",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 1024,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-Guard-4-12B",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 1024,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.19999999999999998, "completion": 0.6 }
					}
				],
				"maxOutput": 1024,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Metas latest safety/guardrail model for prompt and output moderation, aligning conversations to policy via classification and constrained generation.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"top_k",
					"top_p"
				]
			},
			{
				"id": "llama-3.3-70b-instruct",
				"name": "Llama-3.3-70B-Versatile",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "llama-3.3-70b-versatile",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.9e-7,
										"output": 7.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 32678,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "llama-3.3-70b-versatile",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.9e-7,
									"output": 7.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 32678,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.59, "completion": 0.7899999999999999 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-3.3-70b-instruct",
								"pricing": [{ "threshold": 0, "input": 9.5e-7, "output": 0.00000237 }],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-3.3-70b-instruct",
							"pricing": [{ "threshold": 0, "input": 9.5e-7, "output": 0.00000237 }],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.95, "completion": 2.37 }
					}
				],
				"maxOutput": 32678,
				"trainingDate": "2024-12-01T00:00:00.000Z",
				"description": "Flagship 70B instruction-tuned model for high-quality chat, coding, and reasoning with strong instruction-following and multilingual support.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"response_format",
					"seed",
					"top_logprobs"
				]
			},
			{
				"id": "llama-3.1-8b-instant",
				"name": "Llama-3.1-8B-Instant",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "llama-3.1-8b-instant",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 8e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "llama-3.1-8b-instant",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 8e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.049999999999999996, "completion": 0.08 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-3.1-8b-instruct",
								"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 3.1e-7 }],
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-3.1-8b-instruct",
							"pricing": [{ "threshold": 0, "input": 2.1e-7, "output": 3.1e-7 }],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.21, "completion": 0.31 }
					}
				],
				"maxOutput": 32678,
				"trainingDate": "2024-07-01T00:00:00.000Z",
				"description": "Compact 8B general-purpose model offering efficient inference for chat, coding, and RAG workflows on limited compute.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"response_format",
					"seed",
					"top_logprobs"
				]
			},
			{
				"id": "llama-prompt-guard-2-86m",
				"name": "Llama-Prompt-Guard-2-86M",
				"author": "meta-llama",
				"contextLength": 512,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-prompt-guard-2-86m",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-8,
										"output": 1e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 512,
								"maxCompletionTokens": 2,
								"supportedParameters": ["max_tokens", "temperature", "top_p"],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "meta-llama/llama-prompt-guard-2-86m",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-8,
									"output": 1e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 512,
							"maxCompletionTokens": 2,
							"ptbEnabled": true,
							"supportedParameters": ["max_tokens", "temperature", "top_p"]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.01, "completion": 0.01 }
					}
				],
				"maxOutput": 2,
				"trainingDate": "2024-10-01T00:00:00.000Z",
				"description": "86M parameter multilingual prompt safety classifier based on mDeBERTa-base, detecting prompt injections and jailbreaks across 8+ languages with adversarial-resistant tokenization.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": ["max_tokens", "temperature", "top_p"]
			},
			{
				"id": "llama-prompt-guard-2-22m",
				"name": "Llama-Prompt-Guard-2-22M",
				"author": "meta-llama",
				"contextLength": 512,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-prompt-guard-2-22m",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-8,
										"output": 1e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 512,
								"maxCompletionTokens": 2,
								"supportedParameters": ["max_tokens", "temperature", "top_p"],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "meta-llama/llama-prompt-guard-2-22m",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-8,
									"output": 1e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 512,
							"maxCompletionTokens": 2,
							"ptbEnabled": true,
							"supportedParameters": ["max_tokens", "temperature", "top_p"]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.01, "completion": 0.01 }
					}
				],
				"maxOutput": 2,
				"trainingDate": "2024-10-01T00:00:00.000Z",
				"description": "22M parameter lightweight prompt safety classifier based on DeBERTa-xsmall, offering 75% reduced latency for detecting prompt injections and jailbreaks, primarily optimized for English.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": ["max_tokens", "temperature", "top_p"]
			},
			{
				"id": "kimi-k2",
				"name": "Kimi K2 Instruct",
				"author": "moonshotai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"providerModelId": "moonshotai/kimi-k2-instruct",
								"provider": "groq",
								"author": "moonshotai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000001,
										"output": 0.000003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "moonshotai/kimi-k2-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000001,
									"output": 0.000003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1, "completion": 3, "cacheRead": 0.5 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2",
								"pricing": [{ "threshold": 0, "input": 0.00000105, "output": 0.00000316 }],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "moonshotai/kimi-k2",
							"pricing": [{ "threshold": 0, "input": 0.00000105, "output": 0.00000316 }],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.0499999999999998, "completion": 3.1599999999999997 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters.\nTrained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n\nKey Features\nLarge-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\nMuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\nAgentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "kimi-k2-0905",
				"name": "Kimi K2 Instruct (09/05)",
				"author": "moonshotai",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"providerModelId": "moonshotai/kimi-k2-instruct-0905",
								"provider": "groq",
								"author": "moonshotai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000001,
										"output": 0.000003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": { "cachedInput": 0.5 }
									}
								],
								"contextLength": 262144,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "moonshotai/kimi-k2-instruct-0905",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000001,
									"output": 0.000003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": { "cachedInput": 0.5 }
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1, "completion": 3, "cacheRead": 0.5 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2-0905",
								"pricing": [{ "threshold": 0, "input": 0.00000142, "output": 0.00000528 }],
								"contextLength": 262144,
								"maxCompletionTokens": 262144,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "moonshotai/kimi-k2-0905",
							"pricing": [{ "threshold": 0, "input": 0.00000142, "output": 0.00000528 }],
							"contextLength": 262144,
							"maxCompletionTokens": 262144,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 1.42, "completion": 5.28 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-09-05T00:00:00.000Z",
				"description": "Enhanced version of Kimi K2 with doubled context window (256k tokens) and significantly improved coding capabilities, especially for frontend development. Features superior performance on public benchmarks (69.2% on SWE-bench Verified) and more polished code generation for web and 3D scenarios.\n\nKey Improvements\n2x Context Window: Increased from 128k to 256k tokens for better long-horizon task support.\nEnhanced Coding: Specialized training for frontend development, UI code generation, and tool calling.\nAgentic Capabilities: Improved reliability for code generation rivaling frontier closed models.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "qwen3-32b",
				"name": "Qwen3-32B",
				"author": "alibaba",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-32B",
								"provider": "groq",
								"author": "alibaba",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.9e-7,
										"output": 5.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 40960,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "Qwen/Qwen3-32B",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.9e-7,
									"output": 5.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 40960,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.29, "completion": 0.59 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "qwen/qwen3-32b",
								"provider": "openrouter",
								"author": "alibaba",
								"pricing": [{ "threshold": 0, "input": 4.22e-7, "output": 8.44e-7 }],
								"contextLength": 40960,
								"maxCompletionTokens": 40960,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "qwen/qwen3-32b",
							"pricing": [{ "threshold": 0, "input": 4.22e-7, "output": 8.44e-7 }],
							"contextLength": 40960,
							"maxCompletionTokens": 40960,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.422, "completion": 0.844 }
					}
				],
				"maxOutput": 40960,
				"trainingDate": "2025-04-28T00:00:00.000Z",
				"description": "Qwen3-32B is a 32.8 billion parameter language model that uniquely supports seamless switching between thinking mode for complex reasoning tasks and non-thinking mode for efficient general dialogue within a single model. The model excels across 100+ languages with enhanced reasoning capabilities, superior human preference alignment, and strong agent-based task performance, supporting up to 131,072 tokens with YaRN extension.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "qwen3-30b-a3b",
				"name": "Qwen: Qwen3-30B-A3B",
				"author": "qwen",
				"contextLength": 41000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-30B-A3B",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [{ "threshold": 0, "input": 8e-8, "output": 2.9e-7 }],
								"rateLimits": { "rpm": 12000, "tpm": 60000000, "tpd": 6000000000 },
								"contextLength": 32768,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"min_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-30B-A3B",
							"pricing": [{ "threshold": 0, "input": 8e-8, "output": 2.9e-7 }],
							"contextLength": 32768,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"min_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.08, "completion": 0.29 }
					}
				],
				"maxOutput": 41000,
				"trainingDate": "2025-06-01T00:00:00.000Z",
				"description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"min_p"
				]
			},
			{
				"id": "qwen3-coder",
				"name": "Qwen3-Coder-480B-A35B-Instruct-Turbo",
				"author": "qwen",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [{ "threshold": 0, "input": 2.9e-7, "output": 0.0000012 }],
								"rateLimits": { "rpm": 12000, "tpm": 60000000, "tpd": 6000000000 },
								"contextLength": 262144,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo",
							"pricing": [{ "threshold": 0, "input": 2.9e-7, "output": 0.0000012 }],
							"contextLength": 262144,
							"maxCompletionTokens": 16384,
							"ptbEnabled": false,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 0.29, "completion": 1.2 }
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-07-23T00:00:00.000Z",
				"description": "Qwen3-Coder-480B-A35B-Instruct is the Qwen3's most agentic code model, featuring significant performance on agentic coding, agentic browser-use and other foundational coding tasks, achieving results comparable to Claude Sonnet. This model supports multimodal capabilities including text, images, audio, video, and audio-visual reasoning.",
				"inputModalities": ["text", "image", "audio", "video"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "deepseek-r1-distill-llama-70b",
				"name": "DeepSeek-R1-Distill-Llama-70B",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"providerModelId": "deepseek-r1-distill-llama-70b",
								"provider": "groq",
								"author": "deepseek",
								"pricing": [
									{
										"threshold": 0,
										"input": 7.5e-7,
										"output": 9.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "groq",
							"providerModelId": "deepseek-r1-distill-llama-70b",
							"pricing": [
								{
									"threshold": 0,
									"input": 7.5e-7,
									"output": 9.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.75, "completion": 0.9900000000000001 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-r1-distill-llama-70b",
								"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000211 }],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-r1-distill-llama-70b",
							"pricing": [{ "threshold": 0, "input": 0.00000211, "output": 0.00000211 }],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 2.1100000000000003, "completion": 2.1100000000000003 }
					}
				],
				"maxOutput": 4096,
				"trainingDate": "2025-01-20T00:00:00.000Z",
				"description": "DeepSeek-R1-Distill-Llama-70B is a 70-billion parameter model created by distilling the reasoning capabilities of DeepSeek's flagship R1 model (671B parameters) into Meta's Llama-3.3-70B-Instruct base. It achieves exceptional performance on mathematical reasoning and coding benchmarks (94.5% on MATH-500, 1633 CodeForces rating), rivaling OpenAI's o1-mini while being fully open-source under MIT license. The model demonstrates that advanced reasoning patterns from larger models can be effectively transferred to smaller, more deployable architectures through knowledge distillation.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "deepseek-v3",
				"name": "DeepSeek-V3",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "deepseek",
								"providerModelId": "deepseek-ai/DeepSeek-V3.1",
								"pricing": [{ "threshold": 0, "input": 2.7e-7, "output": 0.000001 }],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "deepseek-ai/DeepSeek-V3.1",
							"pricing": [{ "threshold": 0, "input": 2.7e-7, "output": 0.000001 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": false,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 0.27, "completion": 1 }
					},
					{
						"provider": "deepseek",
						"providerSlug": "deepseek",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepseek",
								"author": "deepseek",
								"providerModelId": "deepseek-chat",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.6e-7,
										"output": 0.00000168,
										"cacheMultipliers": { "cachedInput": 0.125 }
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"function_call",
									"functions",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepseek",
							"providerModelId": "deepseek-chat",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.6e-7,
									"output": 0.00000168,
									"cacheMultipliers": { "cachedInput": 0.125 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"function_call",
								"functions",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.56, "completion": 1.68, "cacheRead": 0.07 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-chat-v3.1",
								"pricing": [{ "threshold": 0, "input": 0.00000316, "output": 0.00000475 }],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"function_call",
									"functions",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-chat-v3.1",
							"pricing": [{ "threshold": 0, "input": 0.00000316, "output": 0.00000475 }],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"function_call",
								"functions",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3.1599999999999997, "completion": 4.75 }
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-12-26T00:00:00.000Z",
				"description": "DeepSeek-V3.1 (deepseek-chat) is a powerful generalist model with 671B parameters, offering exceptional performance at an economical price. It achieves strong results on mathematical reasoning, coding, and general language tasks. The model supports 128K context length with a default output of 4K tokens (max 8K) and features advanced capabilities like function calling and JSON output.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"function_call",
					"functions",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"stream",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p",
					"repetition_penalty",
					"top_k",
					"min_p"
				]
			},
			{
				"id": "deepseek-reasoner",
				"name": "DeepSeek-Reasoner",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepseek",
						"providerSlug": "deepseek",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepseek",
								"author": "deepseek",
								"providerModelId": "deepseek-reasoner",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.6e-7,
										"output": 0.00000168,
										"cacheMultipliers": { "cachedInput": 0.125 }
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepseek",
							"providerModelId": "deepseek-reasoner",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.6e-7,
									"output": 0.00000168,
									"cacheMultipliers": { "cachedInput": 0.125 }
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": { "prompt": 0.56, "completion": 1.68, "cacheRead": 0.07 }
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-r1",
								"pricing": [{ "threshold": 0, "input": 0.00000316, "output": 0.00000844 }],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-r1",
							"pricing": [{ "threshold": 0, "input": 0.00000316, "output": 0.00000844 }],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": { "prompt": 3.1599999999999997, "completion": 8.440000000000001 }
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-01-20T00:00:00.000Z",
				"description": "DeepSeek-Reasoner (DeepSeek-V3.1 Thinking Mode) is designed for advanced reasoning, mathematical problem-solving, and complex coding tasks. It uses chain-of-thought reasoning to break down complex problems and achieve superior performance on reasoning benchmarks. Supports 128K context with a default output of 32K tokens (max 64K) for extensive reasoning chains.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"stream",
					"temperature",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "mistral-nemo",
				"name": "Mistral: Mistral-Nemo",
				"author": "mistralai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "mistralai",
							"modelConfig": {
								"providerModelId": "mistralai/Mistral-Nemo-Instruct-2407",
								"provider": "deepinfra",
								"author": "mistralai",
								"pricing": [{ "threshold": 0, "input": 0.02, "output": 0.04 }],
								"rateLimits": { "rpm": 12000, "tpm": 60000000, "tpd": 6000000000 },
								"contextLength": 128000,
								"maxCompletionTokens": 16400,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "mistralai/Mistral-Nemo-Instruct-2407",
							"pricing": [{ "threshold": 0, "input": 0.02, "output": 0.04 }],
							"contextLength": 128000,
							"maxCompletionTokens": 16400,
							"ptbEnabled": false,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 20000, "completion": 40000 }
					}
				],
				"maxOutput": 16400,
				"trainingDate": "2024-07-18T00:00:00.000Z",
				"description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "mistral-small",
				"name": "Mistral: Mistral-Small",
				"author": "mistralai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "mistralai",
							"modelConfig": {
								"providerModelId": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
								"provider": "deepinfra",
								"author": "mistralai",
								"pricing": [{ "threshold": 0, "input": 0.05, "output": 0.1 }],
								"rateLimits": { "rpm": 12000, "tpm": 60000000, "tpd": 6000000000 },
								"contextLength": 128000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": false,
								"endpointConfigs": { "*": {} }
							},
							"userConfig": { "region": "*", "location": "*" },
							"provider": "deepinfra",
							"providerModelId": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
							"pricing": [{ "threshold": 0, "input": 0.05, "output": 0.1 }],
							"contextLength": 128000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": false,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": false,
						"pricing": { "prompt": 50000, "completion": 100000 }
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2024-02-26T00:00:00.000Z",
				"description": "Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks. It supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			}
		],
		"total": 48,
		"filters": {
			"providers": [
				{ "name": "anthropic", "displayName": "Anthropic" },
				{ "name": "azure", "displayName": "Azure OpenAI" },
				{ "name": "bedrock", "displayName": "AWS Bedrock" },
				{ "name": "deepinfra", "displayName": "DeepInfra" },
				{ "name": "deepseek", "displayName": "DeepSeek" },
				{ "name": "google-ai-studio", "displayName": "Google AI Studio" },
				{ "name": "groq", "displayName": "Groq" },
				{ "name": "openai", "displayName": "OpenAI" },
				{ "name": "openrouter", "displayName": "OpenRouter" },
				{ "name": "vertex", "displayName": "Vertex AI" },
				{ "name": "xai", "displayName": "xAI" }
			],
			"authors": [
				"alibaba",
				"anthropic",
				"deepseek",
				"google",
				"meta-llama",
				"mistralai",
				"moonshotai",
				"openai",
				"qwen",
				"xai"
			],
			"capabilities": ["audio", "caching", "image", "web_search"]
		}
	},
	"error": null
}
