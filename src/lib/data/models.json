{
	"data": {
		"models": [
			{
				"id": "claude-opus-4-1",
				"name": "Anthropic: Claude Opus 4.1",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-1-20250805",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250805",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-opus-4-1-20250805",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250805",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-1@20250805",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {
										"providerModelId": "claude-opus-4-1@20250805"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-opus-4-1@20250805",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-opus-4-1-20250805-v1:0",
								"version": "20250805",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-opus-4-1-20250805-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250805",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-opus-4.1",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00001583,
										"output": 0.00007913,
										"web_search": 0.00001
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-opus-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00001583,
									"output": 0.00007913,
									"web_search": 0.00001
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15.83,
							"completion": 79.13,
							"web_search": 10
						}
					}
				],
				"maxOutput": 32000,
				"trainingDate": "2025-08-05T00:00:00.000Z",
				"description": "Our most capable model with the highest level of intelligence and capability. Supports extended thinking, multilingual capabilities, and vision processing. Moderately fast latency with 32,000 max output tokens. Training data cut-off: March 2025. API model name: claude-opus-4-1-20250805",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-opus-4",
				"name": "Anthropic: Claude Opus 4",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4-20250514",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250514",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-opus-4-20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-opus-4@20250514",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {
										"providerModelId": "claude-opus-4@20250514"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-opus-4@20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-opus-4-20250514-v1:0",
								"version": "20250514",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000015,
										"output": 0.000075,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-opus-4-20250514-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000015,
									"output": 0.000075,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15,
							"completion": 75,
							"web_search": 10,
							"cacheRead": 1.5,
							"cacheWrite": 18.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-opus-4",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00001583,
										"output": 0.00007913,
										"web_search": 0.00001
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 32000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-opus-4",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00001583,
									"output": 0.00007913,
									"web_search": 0.00001
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 32000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 15.83,
							"completion": 79.13,
							"web_search": 10
						}
					}
				],
				"maxOutput": 32000,
				"trainingDate": "2025-05-14T00:00:00.000Z",
				"description": "Our previous flagship model with very high intelligence and capability. Supports extended thinking, multilingual capabilities, and vision processing. Moderately fast latency with 32,000 max output tokens. Training data cut-off: March 2025. API model name: claude-opus-4-20250514",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-sonnet-4",
				"name": "Anthropic: Claude Sonnet 4",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-sonnet-4-20250514",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250514",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225,
										"web_search": 0.00001
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-sonnet-4-20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225,
									"web_search": 0.00001
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"web_search": 10,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"web_search": 10,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-sonnet-4@20250514",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225,
										"web_search": 0.00001
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {
										"providerModelId": "claude-sonnet-4@20250514"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-sonnet-4@20250514",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225,
									"web_search": 0.00001
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"web_search": 10,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"web_search": 10,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-sonnet-4-20250514-v1:0",
								"version": "20250514",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225,
										"web_search": 0.00001
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-sonnet-4-20250514-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225,
									"web_search": 0.00001
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250514",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"web_search": 10,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"web_search": 10,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-sonnet-4",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000633,
										"output": 0.00002374,
										"web_search": 0.00001
									}
								],
								"contextLength": 1000000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-sonnet-4",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000633,
									"output": 0.00002374,
									"web_search": 0.00001
								}
							],
							"contextLength": 1000000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 6.33,
							"completion": 23.74,
							"web_search": 10
						}
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-05-14T00:00:00.000Z",
				"description": "High-performance model with high intelligence and balanced performance. Supports extended thinking, multilingual capabilities, and vision processing. Fast latency with 64,000 max output tokens. API model name: claude-sonnet-4-20250514",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "claude-3.7-sonnet",
				"name": "Anthropic: Claude 3.7 Sonnet",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "anthropic",
								"author": "anthropic",
								"providerModelId": "claude-3-7-sonnet-20250219",
								"version": "20250219",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-3-7-sonnet-20250219",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250219",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-3-7-sonnet@20250219",
								"version": "vertex-2023-10-16",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-3-7-sonnet@20250219",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-7-sonnet-20250219-v1:0",
								"version": "20250219",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-7-sonnet-20250219-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250219",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.7-sonnet",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003165,
										"output": 0.00001583
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.7-sonnet",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003165,
									"output": 0.00001583
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3.165,
							"completion": 15.83
						}
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-02-19T00:00:00.000Z",
				"description": "High-performance model with toggleable extended thinking for complex reasoning tasks. Combines high intelligence with the ability to think through problems step-by-step. Fast latency with 64,000 max output tokens. API model name: claude-3-7-sonnet-20250219",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "claude-3.5-sonnet-v2",
				"name": "Anthropic: Claude 3.5 Sonnet v2",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-3-5-sonnet-20241022",
								"provider": "anthropic",
								"author": "anthropic",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-3-5-sonnet-20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-3-5-sonnet-v2@20241022",
								"provider": "vertex",
								"author": "anthropic",
								"version": "vertex-2023-10-16",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {
										"providerModelId": "claude-3-5-sonnet-v2@20241022"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-3-5-sonnet-v2@20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-5-sonnet-20241022-v2:0",
								"version": "20241022",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-5-sonnet-20241022-v2:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "20241022",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 10,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.5-sonnet",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003165,
										"output": 0.00001583
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.5-sonnet",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003165,
									"output": 0.00001583
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3.165,
							"completion": 15.83
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-10-22T00:00:00.000Z",
				"description": "Our previous intelligent model with high level of intelligence and capability. Fast latency with multilingual and vision capabilities, but no extended thinking. 8,192 max output tokens. Training data cut-off: April 2024. Upgraded version API: claude-3-5-sonnet-20241022, Previous version API: claude-3-5-sonnet-20240620",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "claude-3.5-haiku",
				"name": "Anthropic: Claude 3.5 Haiku",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "anthropic",
								"author": "anthropic",
								"providerModelId": "claude-3-5-haiku-20241022",
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"supportedPlugins": ["web"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								},
								"responseFormat": "ANTHROPIC"
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-3-5-haiku-20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"web_search": 10,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-3-5-haiku@20241022",
								"crossRegion": false,
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"us-east5": {}
								},
								"responseFormat": "ANTHROPIC"
							},
							"userConfig": {
								"region": "us-east5",
								"location": "us-east5"
							},
							"provider": "vertex",
							"providerModelId": "claude-3-5-haiku@20241022",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"web_search": 10,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-3-5-haiku-20241022-v1:0",
								"version": "20241022",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-7,
										"output": 0.000004,
										"web_search": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"us-east-1": {}
								},
								"responseFormat": "ANTHROPIC"
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-3-5-haiku-20241022-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-7,
									"output": 0.000004,
									"web_search": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"version": "20241022",
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.7999999999999999,
							"completion": 4,
							"web_search": 10,
							"cacheRead": 0.08,
							"cacheWrite": 1
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-3.5-haiku",
								"pricing": [
									{
										"threshold": 0,
										"input": 8.44e-7,
										"output": 0.00000422
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"top_k",
									"stop",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-3.5-haiku",
							"pricing": [
								{
									"threshold": 0,
									"input": 8.44e-7,
									"output": 0.00000422
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"top_k",
								"stop",
								"tools",
								"tool_choice"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.844,
							"completion": 4.220000000000001
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-10-22T00:00:00.000Z",
				"description": "Our fastest model. Intelligence at blazing speeds. Multilingual and vision capabilities. 8,192 max output tokens. Training data cut-off: July 2024. API model name: claude-3-5-haiku-20241022",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"top_k",
					"stop"
				]
			},
			{
				"id": "claude-4.5-sonnet",
				"name": "Anthropic: Claude Sonnet 4.5",
				"author": "anthropic",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "anthropic",
						"providerSlug": "anthropic",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"providerModelId": "claude-sonnet-4-5-20250929",
								"provider": "anthropic",
								"author": "anthropic",
								"version": "20250929",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25,
											"write1h": 2
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "anthropic",
							"providerModelId": "claude-sonnet-4-5-20250929",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25,
										"write1h": 2
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250929",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "vertex",
								"author": "anthropic",
								"providerModelId": "claude-sonnet-4-5@20250929",
								"version": "vertex-2023-10-16",
								"ptbEnabled": true,
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice"
								],
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"global": {
										"providerModelId": "claude-sonnet-4-5@20250929"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "claude-sonnet-4-5@20250929",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "vertex-2023-10-16",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "bedrock",
						"providerSlug": "aws-bedrock",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "bedrock",
								"author": "anthropic",
								"providerModelId": "anthropic.claude-sonnet-4-5-20250929-v1:0",
								"version": "20250929",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"cacheMultipliers": {
											"cachedInput": 0.1,
											"write5m": 1.25
										}
									},
									{
										"threshold": 200000,
										"input": 0.000006,
										"output": 0.0000225
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"reasoning",
									"include_reasoning",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"responseFormat": "ANTHROPIC",
								"endpointConfigs": {
									"us-east-1": {}
								}
							},
							"userConfig": {
								"region": "us-east-1",
								"location": "us-east-1"
							},
							"provider": "bedrock",
							"providerModelId": "anthropic.claude-sonnet-4-5-20250929-v1:0",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"cacheMultipliers": {
										"cachedInput": 0.1,
										"write5m": 1.25
									}
								},
								{
									"threshold": 200000,
									"input": 0.000006,
									"output": 0.0000225
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"version": "20250929",
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"reasoning",
								"include_reasoning",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"cacheRead": 0.30000000000000004,
							"cacheWrite": 3.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"cacheRead": 0.30000000000000004,
								"cacheWrite": 3.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 22.5,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "anthropic",
							"modelConfig": {
								"provider": "openrouter",
								"author": "anthropic",
								"providerModelId": "anthropic/claude-sonnet-4.5",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000633,
										"output": 0.00002374
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"stop",
									"tools",
									"tool_choice",
									"top_p",
									"top_k"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "anthropic/claude-sonnet-4.5",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000633,
									"output": 0.00002374
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"stop",
								"tools",
								"tool_choice",
								"top_p",
								"top_k"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 6.33,
							"completion": 23.74
						}
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-09-29T00:00:00.000Z",
				"description": "Best-in-class coding and agentic model with hours-long autonomous operation capabilities. Supports extended thinking, context awareness, parallel tool usage, and vision processing. Refined concise communication style with 64,000 max output tokens. API model name: claude-sonnet-4-5-20250929",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"stop",
					"reasoning",
					"include_reasoning",
					"tools",
					"tool_choice",
					"top_p",
					"top_k"
				]
			},
			{
				"id": "gpt-4o",
				"name": "OpenAI GPT-4o",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000025,
										"output": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"rateLimits": {
									"rpm": 10000,
									"tpm": 30000000,
									"tpd": 15000000000
								},
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-4o",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000025,
									"output": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.5,
							"completion": 10,
							"cacheRead": 1.25
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000025,
										"output": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": {
									"rpm": 300,
									"tpm": 50000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "gpt-4o",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000025,
									"output": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.5,
							"completion": 10,
							"cacheRead": 1.25
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4o",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000264,
										"output": 0.00001055
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4o",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000264,
									"output": 0.00001055
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.64,
							"completion": 10.55
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-05-13T00:00:00.000Z",
				"description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4o-mini",
				"name": "OpenAI GPT-4o-mini",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 6e-7,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"rateLimits": {
									"rpm": 30000,
									"tpm": 150000000,
									"tpd": 15000000000
								},
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-4o-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 6e-7,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.15,
							"completion": 0.6,
							"cacheRead": 0.075
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4o-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 6e-7,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": {
									"rpm": 2000,
									"tpm": 200000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "gpt-4o-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 6e-7,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.15,
							"completion": 0.6,
							"cacheRead": 0.075
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4o-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.6e-7,
										"output": 6.3e-7
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4o-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.6e-7,
									"output": 6.3e-7
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.16,
							"completion": 0.63
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-07-18T00:00:00.000Z",
				"description": "GPT-4o mini is OpenAI's newest model after GPT-4 Omni, supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than GPT-3.5 Turbo. It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences common leaderboards.\n\nCheck out the launch announcement to learn more.\n\n#multimodal",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "chatgpt-4o-latest",
				"name": "OpenAI ChatGPT-4o",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/chatgpt-4o-latest",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000528,
										"output": 0.00001582
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/chatgpt-4o-latest",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000528,
									"output": 0.00001582
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 5.28,
							"completion": 15.82
						}
					},
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "chatgpt-4o-latest",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000005,
										"output": 0.00002,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"rateLimits": {
									"rpm": 10000,
									"tpm": 30000000,
									"tpd": 15000000000
								},
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "chatgpt-4o-latest",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000005,
									"output": 0.00002,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 5,
							"completion": 20,
							"cacheRead": 2.5
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-08-14T00:00:00.000Z",
				"description": "OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of GPT-4o in that it has additional RLHF. It is intended for research and evaluation.\n\nOpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "o3",
				"name": "OpenAI o3",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-2025-04-16",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"rateLimits": {
									"rpm": 10000,
									"tpm": 30000000,
									"tpd": 5000000000
								},
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "o3-2025-04-16",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2,
							"completion": 8,
							"cacheRead": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000211,
										"output": 0.00000844
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/o3",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000211,
									"output": 0.00000844
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.1100000000000003,
							"completion": 8.440000000000001
						}
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. o3 is succeeded by GPT-5.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o3-pro",
				"name": "OpenAI o3 Pro",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-pro-2025-06-10",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00002,
										"output": 0.00008
									}
								],
								"rateLimits": {
									"rpm": 10000,
									"tpm": 30000000,
									"tpd": 5000000000
								},
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "o3-pro-2025-06-10",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00002,
									"output": 0.00008
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 20,
							"completion": 80
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3-pro",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000211,
										"output": 0.0000844
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/o3-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000211,
									"output": 0.0000844
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 21.1,
							"completion": 84.4
						}
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers. o3-pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests. Since o3-pro is designed to tackle tough problems, some requests may take several minutes to finish.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o3-mini",
				"name": "OpenAI o3 Mini",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"rateLimits": {
									"rpm": 30000,
									"tpm": 150000000,
									"tpd": 15000000000
								},
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "o3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.1,
							"completion": 4.4,
							"cacheRead": 0.55
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o3-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"rateLimits": {
									"rpm": 20,
									"tpm": 200000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "o3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.1,
							"completion": 4.4,
							"cacheRead": 0.55
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o3-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000116,
										"output": 0.00000464
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/o3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000116,
									"output": 0.00000464
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.16,
							"completion": 4.64
						}
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2023-10-01T00:00:00.000Z",
				"description": "o3-mini is our newest small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini. o3-mini supports key developer features, like Structured Outputs, function calling, and Batch API. This model supports the `reasoning_effort` parameter, which can be set to 'high', 'medium', or 'low' to control the thinking time of the model.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "o4-mini",
				"name": "OpenAI o4 Mini",
				"author": "openai",
				"contextLength": 200000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o4-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"rateLimits": {
									"rpm": 30000,
									"tpm": 150000000,
									"tpd": 15000000000
								},
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "o4-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.1,
							"completion": 4.4,
							"cacheRead": 0.275
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "o4-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.0000011,
										"output": 0.0000044,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"rateLimits": {
									"rpm": 20,
									"tpm": 20000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "o4-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.0000011,
									"output": 0.0000044,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.1,
							"completion": 4.4,
							"cacheRead": 0.275
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/o4-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000116,
										"output": 0.00000464
									}
								],
								"contextLength": 200000,
								"maxCompletionTokens": 100000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/o4-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000116,
									"output": 0.00000464
								}
							],
							"contextLength": 200000,
							"maxCompletionTokens": 100000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.16,
							"completion": 4.64
						}
					}
				],
				"maxOutput": 100000,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "o4-mini is our latest small o-series model. It's optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It's succeeded by GPT-5 mini.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format"
				]
			},
			{
				"id": "gpt-4.1",
				"name": "OpenAI GPT-4.1",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 10000,
									"tpm": 30000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2,
							"completion": 8,
							"cacheRead": 0.5
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000002,
										"output": 0.000008,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 50,
									"tpm": 50000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "gpt-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000002,
									"output": 0.000008,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2,
							"completion": 8,
							"cacheRead": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000211,
										"output": 0.00000844
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000211,
									"output": 0.00000844
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.1100000000000003,
							"completion": 8.440000000000001
						}
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:23:05.000Z",
				"description": "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4.1-mini",
				"name": "OpenAI GPT-4.1 Mini",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 4e-7,
										"output": 0.0000016,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 30000,
									"tpm": 150000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-4.1-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 4e-7,
									"output": 0.0000016,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.39999999999999997,
							"completion": 1.5999999999999999,
							"cacheRead": 0.09999999999999999
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-mini",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 4e-7,
										"output": 0.0000016,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 200,
									"tpm": 200000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "gpt-4.1-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 4e-7,
									"output": 0.0000016,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.39999999999999997,
							"completion": 1.5999999999999999,
							"cacheRead": 0.09999999999999999
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 4.2e-7,
										"output": 0.00000169
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 4.2e-7,
									"output": 0.00000169
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.42,
							"completion": 1.69
						}
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:23:01.000Z",
				"description": "GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider's polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-4.1-nano",
				"name": "OpenAI GPT-4.1 Nano",
				"author": "openai",
				"contextLength": 1047576,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-nano",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 30000,
									"tpm": 150000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-4.1-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"cacheRead": 0.024999999999999998
						}
					},
					{
						"provider": "azure",
						"providerSlug": "azure",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-4.1-nano",
								"provider": "azure",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"cacheMultipliers": {
											"cachedInput": 0.3
										}
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"rateLimits": {
									"rpm": 200,
									"tpm": 200000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "azure",
							"providerModelId": "gpt-4.1-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"cacheMultipliers": {
										"cachedInput": 0.3
									}
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"cacheRead": 0.03
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-4.1-nano",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.1e-7,
										"output": 4.2e-7
									}
								],
								"contextLength": 1047576,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-4.1-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.1e-7,
									"output": 4.2e-7
								}
							],
							"contextLength": 1047576,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.11,
							"completion": 0.42
						}
					}
				],
				"maxOutput": 32768,
				"trainingDate": "2025-04-14T17:22:49.000Z",
				"description": "For tasks that demand low latency, GPT-4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding  even higher than GPT-4o mini. It's ideal for tasks like classification or autocompletion.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty"
				]
			},
			{
				"id": "gpt-5",
				"name": "OpenAI GPT-5",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1
										}
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": {
									"rpm": 15000,
									"tpm": 40000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-5",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1
									}
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"cacheRead": 0.12500000000000003
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000132,
										"output": 0.00001055
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000132,
									"output": 0.00001055
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.32,
							"completion": 10.55
						}
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 is OpenAI's most advanced language model, featuring enhanced reasoning capabilities with 80% fewer factual errors than o3. It supports a 400K total context (272K input + 128K output), advanced tool calling with reliable chaining of dozens of calls, and a new verbosity parameter for response length control. Ideal for complex reasoning, multi-step planning, and applications requiring high accuracy.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-mini",
				"name": "OpenAI GPT-5 Mini",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-mini",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.5e-7,
										"output": 0.000002,
										"cacheMultipliers": {
											"cachedInput": 0.1
										}
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": {
									"rpm": 30000,
									"tpm": 180000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-5-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.5e-7,
									"output": 0.000002,
									"cacheMultipliers": {
										"cachedInput": 0.1
									}
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.25,
							"completion": 2,
							"cacheRead": 0.024999999999999998
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.6e-7,
										"output": 0.00000211
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.6e-7,
									"output": 0.00000211
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.26,
							"completion": 2.1100000000000003
						}
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 Mini delivers GPT-5-level performance at a fraction of the cost and latency. With the same 400K context window and advanced capabilities including tool calling and verbosity control, it's optimized for speed and efficiency while maintaining strong reasoning and instruction-following capabilities. Perfect for high-volume applications requiring advanced AI capabilities with resource constraints.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-nano",
				"name": "OpenAI GPT-5 Nano",
				"author": "openai",
				"contextLength": 400000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-nano",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 4e-7,
										"cacheMultipliers": {
											"cachedInput": 0.1
										}
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"rateLimits": {
									"rpm": 30000,
									"tpm": 180000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-5-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 4e-7,
									"cacheMultipliers": {
										"cachedInput": 0.1
									}
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.39999999999999997,
							"cacheRead": 0.005
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-nano",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 4.2e-7
									}
								],
								"contextLength": 400000,
								"maxCompletionTokens": 128000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-nano",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 4.2e-7
								}
							],
							"contextLength": 400000,
							"maxCompletionTokens": 128000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.42
						}
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "GPT-5 Nano is the smallest and fastest model in the GPT-5 family, designed for ultra-low latency applications. Despite its compact size, it maintains the full 400K context window and delivers impressive performance on classification, completion, and simple reasoning tasks. Ideal for real-time applications, edge deployments, and high-throughput scenarios.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-5-chat-latest",
				"name": "OpenAI GPT-5 Chat Latest",
				"author": "openai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "openai",
						"providerSlug": "openai",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "gpt-5-chat-latest",
								"provider": "openai",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"cacheMultipliers": {
											"cachedInput": 0.1
										}
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"rateLimits": {
									"rpm": 15000,
									"tpm": 40000000,
									"tpd": 15000000000
								},
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"verbosity"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openai",
							"providerModelId": "gpt-5-chat-latest",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"cacheMultipliers": {
										"cachedInput": 0.1
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"verbosity"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"cacheRead": 0.12500000000000003
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-5-chat",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000132,
										"output": 0.00001055
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_completion_tokens",
									"response_format",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty"
								],
								"unsupportedParameters": ["temperature"],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-5-chat",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000132,
									"output": 0.00001055
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_completion_tokens",
								"response_format",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.32,
							"completion": 10.55
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-09-30T00:00:00.000Z",
				"description": "GPT-5 Chat Latest is a continuously updated version of GPT-5 optimized for conversational interactions. It receives regular updates with the latest improvements in dialogue management, safety, and helpfulness. Features a 128K context window and 16K max output tokens, making it ideal for focused conversations. Knowledge cutoff: September 30, 2024.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_completion_tokens",
					"response_format",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"verbosity"
				]
			},
			{
				"id": "gpt-oss-120b",
				"name": "OpenAI GPT-OSS 120b",
				"author": "openai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-120b",
								"provider": "deepinfra",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 4e-8,
										"output": 1.6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"quantization": "fp4",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [
								{
									"threshold": 0,
									"input": 4e-8,
									"output": 1.6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.04,
							"completion": 0.16
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-120b",
								"provider": "groq",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 7.5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_completion_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 7.5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_completion_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.15,
							"completion": 0.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-oss-120b",
								"pricing": [
									{
										"threshold": 0,
										"input": 3.7e-7,
										"output": 7.9e-7
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-oss-120b",
							"pricing": [
								{
									"threshold": 0,
									"input": 3.7e-7,
									"output": 7.9e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.37,
							"completion": 0.7899999999999999
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "gpt-oss-120b is our most powerful open-weight model, which fits into a single H100 GPU (117B parameters with 5.1B active parameters). Features permissive Apache 2.0 license, configurable reasoning effort (low, medium, high), full chain-of-thought access, fine-tunable parameters, and agentic capabilities including function calling, web browsing, Python code execution, and structured outputs.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_completion_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"max_tokens"
				]
			},
			{
				"id": "gpt-oss-20b",
				"name": "OpenAI GPT-OSS 20b",
				"author": "openai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "novita",
								"author": "openai",
								"providerModelId": "openai/gpt-oss-20b",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 2e-7
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 32768,
								"quantization": "bf16",
								"supportedParameters": [
									"structured_outputs",
									"reasoning",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "openai/gpt-oss-20b",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 2e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"structured_outputs",
								"reasoning",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.19999999999999998
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"providerModelId": "openai/gpt-oss-20b",
								"provider": "groq",
								"author": "openai",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_completion_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "openai/gpt-oss-20b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_completion_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "openai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "openai",
								"providerModelId": "openai/gpt-oss-20b",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.1e-7,
										"output": 5.3e-7
									}
								],
								"contextLength": 131000,
								"maxCompletionTokens": 131000,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "openai/gpt-oss-20b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.1e-7,
									"output": 5.3e-7
								}
							],
							"contextLength": 131000,
							"maxCompletionTokens": 131000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.11,
							"completion": 0.53
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "gpt-oss-20b is our medium-sized open-weight model for low latency, local, or specialized use-cases (21B parameters with 3.6B active parameters). Features permissive Apache 2.0 license, configurable reasoning effort (low, medium, high), full chain-of-thought access, fine-tunable parameters, and agentic capabilities including function calling, web browsing, Python code execution, and structured outputs.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_completion_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"max_tokens"
				]
			},
			{
				"id": "gemini-2.5-pro",
				"name": "Google Gemini 2.5 Pro",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-pro",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"image": 0.00516,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.0000045
									},
									{
										"threshold": 200000,
										"input": 0.0000025,
										"output": 0.000015,
										"image": 0.00516
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": {
									"rpm": 2000,
									"tpm": 8000000
								},
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"image": 0.00516,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.0000045
								},
								{
									"threshold": 200000,
									"input": 0.0000025,
									"output": 0.000015,
									"image": 0.00516
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"image": 5160,
							"cacheRead": 0.3125,
							"cacheWrite": 1.25
						},
						"pricingTiers": [
							{
								"prompt": 1.25,
								"completion": 10,
								"image": 5160,
								"cacheRead": 0.3125,
								"cacheWrite": 1.25,
								"threshold": 0
							},
							{
								"prompt": 2.5,
								"completion": 15,
								"image": 5160,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-pro",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000125,
										"output": 0.00001,
										"image": 0.00516,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.0000045
									},
									{
										"threshold": 200000,
										"input": 0.0000025,
										"output": 0.000015,
										"image": 0.00516
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"global": {
										"providerModelId": "gemini-2.5-pro"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "gemini-2.5-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000125,
									"output": 0.00001,
									"image": 0.00516,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.0000045
								},
								{
									"threshold": 200000,
									"input": 0.0000025,
									"output": 0.000015,
									"image": 0.00516
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.25,
							"completion": 10,
							"image": 5160,
							"cacheRead": 0.3125,
							"cacheWrite": 1.25
						},
						"pricingTiers": [
							{
								"prompt": 1.25,
								"completion": 10,
								"image": 5160,
								"cacheRead": 0.3125,
								"cacheWrite": 1.25,
								"threshold": 0
							},
							{
								"prompt": 2.5,
								"completion": 15,
								"image": 5160,
								"threshold": 200000
							}
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-pro",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000264,
										"output": 0.00001582
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-pro",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000264,
									"output": 0.00001582
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.64,
							"completion": 15.82
						}
					}
				],
				"maxOutput": 65536,
				"trainingDate": "2025-06-17T07:12:24",
				"description": "Gemini 2.5 Pro is Googles state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs thinking capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemini-2.5-flash",
				"name": "Google Gemini 2.5 Flash",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000025,
										"image": 0.001238,
										"audio": 0.000001,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": {
									"rpm": 10000,
									"tpm": 8000000
								},
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-flash",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000025,
									"image": 0.001238,
									"audio": 0.000001,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 2.5,
							"audio": 1,
							"image": 1238,
							"cacheRead": 0.075,
							"cacheWrite": 0.3
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000025,
										"image": 0.001238,
										"audio": 0.000001,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"global": {
										"providerModelId": "gemini-2.5-flash"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "gemini-2.5-flash",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000025,
									"image": 0.001238,
									"audio": 0.000001,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 2.5,
							"audio": 1,
							"image": 1238,
							"cacheRead": 0.075,
							"cacheWrite": 0.3
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-flash",
								"pricing": [
									{
										"threshold": 0,
										"input": 3.2e-7,
										"output": 0.00000264
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-flash",
							"pricing": [
								{
									"threshold": 0,
									"input": 3.2e-7,
									"output": 0.00000264
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.32,
							"completion": 2.64
						}
					}
				],
				"maxOutput": 65535,
				"trainingDate": "2025-06-17T08:01:28",
				"description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, allowing fine-tuned control over its reasoning process.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemini-2.5-flash-lite",
				"name": "Google Gemini 2.5 Flash Lite",
				"author": "google",
				"contextLength": 1048576,
				"endpoints": [
					{
						"provider": "google-ai-studio",
						"providerSlug": "google-ai-studio",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash-lite",
								"provider": "google-ai-studio",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"audio": 3e-7,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"rateLimits": {
									"rpm": 30000,
									"tpm": 30000000
								},
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "google-ai-studio",
							"providerModelId": "gemini-2.5-flash-lite",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"audio": 3e-7,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"audio": 0.3,
							"cacheRead": 0.024999999999999998,
							"cacheWrite": 0.09999999999999999
						}
					},
					{
						"provider": "vertex",
						"providerSlug": "google-vertex",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemini-2.5-flash-lite",
								"provider": "vertex",
								"author": "google",
								"crossRegion": true,
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 4e-7,
										"audio": 3e-7,
										"cacheMultipliers": {
											"cachedInput": 0.25,
											"write5m": 1
										},
										"cacheStoragePerHour": 0.000001
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"include_reasoning",
									"max_tokens",
									"reasoning",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"global": {
										"providerModelId": "gemini-2.5-flash-lite"
									}
								}
							},
							"userConfig": {
								"region": "global",
								"location": "global"
							},
							"provider": "vertex",
							"providerModelId": "gemini-2.5-flash-lite",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 4e-7,
									"audio": 3e-7,
									"cacheMultipliers": {
										"cachedInput": 0.25,
										"write5m": 1
									},
									"cacheStoragePerHour": 0.000001
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"include_reasoning",
								"max_tokens",
								"reasoning",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.39999999999999997,
							"audio": 0.3,
							"cacheRead": 0.024999999999999998,
							"cacheWrite": 0.09999999999999999
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemini-2.5-flash-lite",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.1e-7,
										"output": 4.2e-7
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 65535,
								"supportedParameters": [
									"max_tokens",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "google/gemini-2.5-flash-lite",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.1e-7,
									"output": 4.2e-7
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 65535,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.11,
							"completion": 0.42
						}
					}
				],
				"maxOutput": 65535,
				"trainingDate": "2025-07-22T09:04:36",
				"description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"include_reasoning",
					"max_tokens",
					"reasoning",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_p"
				]
			},
			{
				"id": "gemma2-9b-it",
				"name": "Google Gemma 2",
				"author": "google",
				"contextLength": 8192,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "gemma2-9b-it",
								"provider": "groq",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 2e-7,
										"image": 0
									}
								],
								"contextLength": 8192,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "gemma2-9b-it",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 2e-7,
									"image": 0
								}
							],
							"contextLength": 8192,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.19999999999999998
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"provider": "openrouter",
								"author": "google",
								"providerModelId": "google/gemma-2-9b-it",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.1e-7,
										"output": 2.1e-7
									}
								],
								"contextLength": 8192,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "google/gemma-2-9b-it",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.1e-7,
									"output": 2.1e-7
								}
							],
							"contextLength": 8192,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.21,
							"completion": 0.21
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-06-25T00:00:00.000Z",
				"description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"seed",
					"stop",
					"temperature",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "gemma-3-12b-it",
				"name": "Google Gemma 3 12B",
				"author": "google",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "google",
							"modelConfig": {
								"providerModelId": "google/gemma-3-12b-it",
								"provider": "deepinfra",
								"author": "google",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 1e-7
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"quantization": "bf16",
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "google/gemma-3-12b-it",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 1e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.09999999999999999
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-12-01T00:00:00.000Z",
				"description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "grok-code-fast-1",
				"name": "xAI Grok Code Fast 1",
				"author": "xai",
				"contextLength": 256000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-code-fast-1",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 0.0000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": {
											"cachedInput": 0.1
										}
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 10000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-code-fast-1",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 0.0000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": {
										"cachedInput": 0.1
									}
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 10000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 1.5,
							"cacheRead": 0.02
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-code-fast-1",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.1e-7,
										"output": 0.00000158
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 10000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-code-fast-1",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.1e-7,
									"output": 0.00000158
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 10000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.21,
							"completion": 1.5799999999999998
						}
					}
				],
				"maxOutput": 10000,
				"trainingDate": "2024-08-25T00:00:00.000Z",
				"description": "Speedy and economical reasoning model that excels at agentic coding. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-4",
				"name": "xAI Grok 4",
				"author": "xai",
				"contextLength": 256000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-4",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									},
									{
										"threshold": 128000,
										"input": 0.000006,
										"output": 0.00003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.125
										}
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 256000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-4",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								},
								{
									"threshold": 128000,
									"input": 0.000006,
									"output": 0.00003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.125
									}
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 256000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 25000,
							"cacheRead": 0.75
						},
						"pricingTiers": [
							{
								"prompt": 3,
								"completion": 15,
								"web_search": 25000,
								"cacheRead": 0.75,
								"threshold": 0
							},
							{
								"prompt": 6,
								"completion": 30,
								"web_search": 25000,
								"cacheRead": 0.75,
								"threshold": 128000
							}
						]
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-4",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000633,
										"output": 0.00003165
									}
								],
								"contextLength": 256000,
								"maxCompletionTokens": 256000,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-4",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000633,
									"output": 0.00003165
								}
							],
							"contextLength": 256000,
							"maxCompletionTokens": 256000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 6.33,
							"completion": 31.65
						}
					}
				],
				"maxOutput": 256000,
				"trainingDate": "2024-07-09T00:00:00.000Z",
				"description": "Latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-4-fast-reasoning",
				"name": "xAI: Grok 4 Fast Reasoning",
				"author": "xai",
				"contextLength": 2000000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-4-fast",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 5e-7,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									},
									{
										"threshold": 128000,
										"input": 4e-7,
										"output": 0.000001,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.125
										}
									}
								],
								"contextLength": 2000000,
								"maxCompletionTokens": 2000000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"seed",
									"logprobs",
									"top_logprobs",
									"reasoning"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-4-fast",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 5e-7,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								},
								{
									"threshold": 128000,
									"input": 4e-7,
									"output": 0.000001,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.125
									}
								}
							],
							"contextLength": 2000000,
							"maxCompletionTokens": 2000000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"seed",
								"logprobs",
								"top_logprobs",
								"reasoning"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.5,
							"web_search": 25000,
							"cacheRead": 0.049999999999999996
						},
						"pricingTiers": [
							{
								"prompt": 0.19999999999999998,
								"completion": 0.5,
								"web_search": 25000,
								"cacheRead": 0.049999999999999996,
								"threshold": 0
							},
							{
								"prompt": 0.39999999999999997,
								"completion": 1,
								"web_search": 25000,
								"cacheRead": 0.049999999999999996,
								"threshold": 128000
							}
						]
					}
				],
				"maxOutput": 2000000,
				"trainingDate": "2025-09-01T00:00:00.000Z",
				"description": "Grok 4 Fast is xAI's latest advancement in cost-efficient reasoning models. Built on xAIs learnings from Grok 4, Grok 4 Fast delivers frontier-level performance across Enterprise and Consumer domainswith exceptional token efficiency. This model pushes the boundaries for smaller and faster AI, making high-quality reasoning accessible to more users and developers. Grok 4 Fast features state-of-the-art (SOTA) cost-efficiency, cutting-edge web and X search capabilities, a 2M token context window, and a unified architecture that blends reasoning and non-reasoning modes in one model.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"structured_outputs",
					"response_format",
					"max_tokens",
					"temperature",
					"top_p",
					"seed",
					"logprobs",
					"top_logprobs",
					"reasoning"
				]
			},
			{
				"id": "grok-4-fast-non-reasoning",
				"name": "xAI Grok 4 Fast Non-Reasoning",
				"author": "xai",
				"contextLength": 2000000,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-4-fast-non-reasoning",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 5e-7,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									},
									{
										"threshold": 128000,
										"input": 4e-7,
										"output": 0.000001,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.125
										}
									}
								],
								"contextLength": 2000000,
								"maxCompletionTokens": 2000000,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"seed",
									"logprobs",
									"top_logprobs"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-4-fast-non-reasoning",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 5e-7,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								},
								{
									"threshold": 128000,
									"input": 4e-7,
									"output": 0.000001,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.125
									}
								}
							],
							"contextLength": 2000000,
							"maxCompletionTokens": 2000000,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"seed",
								"logprobs",
								"top_logprobs"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.5,
							"web_search": 25000,
							"cacheRead": 0.049999999999999996
						},
						"pricingTiers": [
							{
								"prompt": 0.19999999999999998,
								"completion": 0.5,
								"web_search": 25000,
								"cacheRead": 0.049999999999999996,
								"threshold": 0
							},
							{
								"prompt": 0.39999999999999997,
								"completion": 1,
								"web_search": 25000,
								"cacheRead": 0.049999999999999996,
								"threshold": 128000
							}
						]
					}
				],
				"maxOutput": 2000000,
				"trainingDate": "2025-09-19T00:00:00.000Z",
				"description": "Grok 4 Fast is xAI's latest advancement in cost-efficient reasoning models. Built on xAIs learnings from Grok 4, Grok 4 Fast delivers frontier-level performance across Enterprise and Consumer domainswith exceptional token efficiency. This model pushes the boundaries for smaller and faster AI, making high-quality reasoning accessible to more users and developers. Grok 4 Fast features state-of-the-art (SOTA) cost-efficiency, cutting-edge web and X search capabilities, a 2M token context window. This non-reasoning model specifically excludes reasoning capabilities.",
				"inputModalities": ["text", "image", "audio"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"structured_outputs",
					"response_format",
					"max_tokens",
					"temperature",
					"top_p",
					"seed",
					"logprobs",
					"top_logprobs"
				]
			},
			{
				"id": "grok-3",
				"name": "xAI Grok 3",
				"author": "xai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-3",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000003,
										"output": 0.000015,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-3",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000003,
									"output": 0.000015,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3,
							"completion": 15,
							"web_search": 25000,
							"cacheRead": 0.75
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-3",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000528,
										"output": 0.00002638
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-3",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000528,
									"output": 0.00002638
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 5.28,
							"completion": 26.380000000000003
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "Excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "grok-3-mini",
				"name": "xAI Grok 3 Mini",
				"author": "xai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "xai",
						"providerSlug": "xai",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"providerModelId": "grok-3-mini",
								"provider": "xai",
								"author": "xai",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 5e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0.025,
										"cacheMultipliers": {
											"cachedInput": 0.25
										}
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "xai",
							"providerModelId": "grok-3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 5e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0.025,
									"cacheMultipliers": {
										"cachedInput": 0.25
									}
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 0.5,
							"web_search": 25000,
							"cacheRead": 0.075
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "xai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "xai",
								"providerModelId": "x-ai/grok-3-mini",
								"pricing": [
									{
										"threshold": 0,
										"input": 6.33e-7,
										"output": 0.00000422
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "x-ai/grok-3-mini",
							"pricing": [
								{
									"threshold": 0,
									"input": 6.33e-7,
									"output": 0.00000422
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.633,
							"completion": 4.220000000000001
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-06-01T00:00:00.000Z",
				"description": "Lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. Features function calling, structured outputs, and reasoning capabilities.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "llama-4-scout",
				"name": "Meta Llama 4 Scout 17B 16E",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "meta-llama",
								"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-8,
										"output": 3e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 10000000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format",
									"tools",
									"functions",
									"tool_choice"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-8,
									"output": 3e-7
								}
							],
							"contextLength": 10000000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format",
								"tools",
								"functions",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.08,
							"completion": 0.3
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.1e-7,
										"output": 3.4e-7,
										"request": 0,
										"image": 0.00036762,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.1e-7,
									"output": 3.4e-7,
									"request": 0,
									"image": 0.00036762,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.11,
							"completion": 0.33999999999999997,
							"image": 367.62
						}
					},
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "novita",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-4-scout-17b-16e-instruct",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 5e-7
									}
								],
								"quantization": "bf16",
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias",
									"functions",
									"tools"
								],
								"priority": 2,
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "meta-llama/llama-4-scout-17b-16e-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 5e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias",
								"functions",
								"tools"
							],
							"priority": 2
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-4-scout",
								"pricing": [
									{
										"threshold": 0,
										"input": 6.9e-7,
										"output": 9e-7
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 1048576,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-4-scout",
							"pricing": [
								{
									"threshold": 0,
									"input": 6.9e-7,
									"output": 9e-7
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 1048576,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.69,
							"completion": 0.8999999999999999
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Llama 4 instruction-tuned MoE (17B, 16 experts) for fast, high-quality chat, tool use, and multilingual reasoning with balanced latency and cost.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"frequency_penalty",
					"presence_penalty",
					"min_p",
					"logit_bias",
					"functions"
				]
			},
			{
				"id": "llama-4-maverick",
				"name": "Meta Llama 4 Maverick 17B 128E",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
								"provider": "deepinfra",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.5e-7,
										"output": 6e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 1000000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"logit_bias",
									"tools",
									"tool_choice",
									"functions"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.5e-7,
									"output": 6e-7
								}
							],
							"contextLength": 1000000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"logit_bias",
								"tools",
								"tool_choice",
								"functions"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.15,
							"completion": 0.6
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.6
						}
					},
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
								"provider": "novita",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.7e-7,
										"output": 8.5e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 1048576,
								"maxCompletionTokens": 1048576,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias",
									"functions",
									"tools",
									"tool_choice"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.7e-7,
									"output": 8.5e-7
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 1048576,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias",
								"functions",
								"tools",
								"tool_choice"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.16999999999999998,
							"completion": 0.85
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-4-maverick",
								"pricing": [
									{
										"threshold": 0,
										"input": 6.6e-7,
										"output": 0.0000019
									}
								],
								"contextLength": 1048576,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-4-maverick",
							"pricing": [
								{
									"threshold": 0,
									"input": 6.6e-7,
									"output": 0.0000019
								}
							],
							"contextLength": 1048576,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.66,
							"completion": 1.9
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Llama 4 instruction-tuned MoE (17B, 128 experts) targeting tougher reasoning and long-form tasks, trading more compute for higher response diversity and robustness.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"frequency_penalty",
					"presence_penalty",
					"min_p",
					"logit_bias",
					"functions"
				]
			},
			{
				"id": "llama-guard-4",
				"name": "Meta Llama Guard 4 12B",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-guard-4-12b",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.1e-7,
										"output": 2.1e-7
									}
								],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-guard-4-12b",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.1e-7,
									"output": 2.1e-7
								}
							],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.21,
							"completion": 0.21
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Llama-Guard-4-12B",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 6e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 1024,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "meta-llama/Llama-Guard-4-12B",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 6e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 1024,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.6
						}
					}
				],
				"maxOutput": 1024,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Metas latest safety/guardrail model for prompt and output moderation, aligning conversations to policy via classification and constrained generation.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"top_k",
					"top_p"
				]
			},
			{
				"id": "llama-3.3-70b-instruct",
				"name": "Meta Llama 3.3 70B Versatile",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-3.3-70b-instruct",
								"provider": "novita",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.3e-7,
										"output": 3.9e-7
									}
								],
								"quantization": "bf16",
								"contextLength": 131072,
								"maxCompletionTokens": 120000,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"tool_choice",
									"tools"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "meta-llama/llama-3.3-70b-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.3e-7,
									"output": 3.9e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 120000,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"tool_choice",
								"tools"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.13,
							"completion": 0.39
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "llama-3.3-70b-versatile",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.9e-7,
										"output": 7.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 32678,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "llama-3.3-70b-versatile",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.9e-7,
									"output": 7.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 32678,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.59,
							"completion": 0.7899999999999999
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-3.3-70b-instruct",
								"pricing": [
									{
										"threshold": 0,
										"input": 9.5e-7,
										"output": 0.00000237
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-3.3-70b-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 9.5e-7,
									"output": 0.00000237
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.95,
							"completion": 2.37
						}
					}
				],
				"maxOutput": 32678,
				"trainingDate": "2024-12-01T00:00:00.000Z",
				"description": "Flagship 70B instruction-tuned model for high-quality chat, coding, and reasoning with strong instruction-following and multilingual support.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"response_format",
					"seed",
					"top_logprobs"
				]
			},
			{
				"id": "llama-3.1-8b-instant",
				"name": "Meta Llama 3.1 8B Instant",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "llama-3.1-8b-instant",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-8,
										"output": 8e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "llama-3.1-8b-instant",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-8,
									"output": 8e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.049999999999999996,
							"completion": 0.08
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"provider": "openrouter",
								"author": "meta-llama",
								"providerModelId": "meta-llama/llama-3.1-8b-instruct",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.1e-7,
										"output": 3.1e-7
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"max_tokens",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "meta-llama/llama-3.1-8b-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.1e-7,
									"output": 3.1e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"max_tokens",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.21,
							"completion": 0.31
						}
					}
				],
				"maxOutput": 32678,
				"trainingDate": "2024-07-01T00:00:00.000Z",
				"description": "Compact 8B general-purpose model offering efficient inference for chat, coding, and RAG workflows on limited compute.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_p",
					"response_format",
					"seed",
					"top_logprobs"
				]
			},
			{
				"id": "llama-prompt-guard-2-86m",
				"name": "Meta Llama Prompt Guard 2 86M",
				"author": "meta-llama",
				"contextLength": 512,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-prompt-guard-2-86m",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-8,
										"output": 1e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 512,
								"maxCompletionTokens": 2,
								"supportedParameters": ["max_tokens", "temperature", "top_p"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "meta-llama/llama-prompt-guard-2-86m",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-8,
									"output": 1e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 512,
							"maxCompletionTokens": 2,
							"ptbEnabled": true,
							"supportedParameters": ["max_tokens", "temperature", "top_p"]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.01,
							"completion": 0.01
						}
					}
				],
				"maxOutput": 2,
				"trainingDate": "2024-10-01T00:00:00.000Z",
				"description": "86M parameter multilingual prompt safety classifier based on mDeBERTa-base, detecting prompt injections and jailbreaks across 8+ languages with adversarial-resistant tokenization.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": ["max_tokens", "temperature", "top_p"]
			},
			{
				"id": "llama-prompt-guard-2-22m",
				"name": "Meta Llama Prompt Guard 2 22M",
				"author": "meta-llama",
				"contextLength": 512,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-prompt-guard-2-22m",
								"provider": "groq",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-8,
										"output": 1e-8,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 512,
								"maxCompletionTokens": 2,
								"supportedParameters": ["max_tokens", "temperature", "top_p"],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "meta-llama/llama-prompt-guard-2-22m",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-8,
									"output": 1e-8,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 512,
							"maxCompletionTokens": 2,
							"ptbEnabled": true,
							"supportedParameters": ["max_tokens", "temperature", "top_p"]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.01,
							"completion": 0.01
						}
					}
				],
				"maxOutput": 2,
				"trainingDate": "2024-10-01T00:00:00.000Z",
				"description": "22M parameter lightweight prompt safety classifier based on DeBERTa-xsmall, offering 75% reduced latency for detecting prompt injections and jailbreaks, primarily optimized for English.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": ["max_tokens", "temperature", "top_p"]
			},
			{
				"id": "llama-3.1-8b-instruct",
				"name": "Meta Llama 3.1 8B Instruct",
				"author": "meta-llama",
				"contextLength": 16384,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/llama-3.1-8b-instruct",
								"provider": "novita",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-8,
										"output": 5e-8
									}
								],
								"quantization": "fp8",
								"contextLength": 16384,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "meta-llama/llama-3.1-8b-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-8,
									"output": 5e-8
								}
							],
							"contextLength": 16384,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.02,
							"completion": 0.049999999999999996
						}
					},
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct",
								"provider": "deepinfra",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-8,
										"output": 5e-8
									}
								],
								"quantization": "bf16",
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format",
									"tool_choice",
									"tools"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-8,
									"output": 5e-8
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format",
								"tool_choice",
								"tools"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.03,
							"completion": 0.049999999999999996
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2024-07-23T00:00:00.000Z",
				"description": "Meta's latest class of models, Llama 3.1, launched with a variety of sizes and configurations. The 8B instruct-tuned version is particularly fast and efficient. It has demonstrated strong performance in human evaluations, outperforming several leading closed-source models.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"min_p",
					"repetition_penalty",
					"logit_bias",
					"response_format",
					"tool_choice",
					"tools"
				]
			},
			{
				"id": "llama-3.1-8b-instruct-turbo",
				"name": "Meta Llama 3.1 8B Instruct Turbo",
				"author": "meta-llama",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
								"provider": "deepinfra",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-8,
										"output": 3e-8
									}
								],
								"quantization": "fp8",
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format",
									"tool_choice",
									"tools"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-8,
									"output": 3e-8
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format",
								"tool_choice",
								"tools"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.02,
							"completion": 0.03
						}
					},
					{
						"provider": "nebius",
						"providerSlug": "nebius",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct-fast",
								"provider": "nebius",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-8,
										"output": 9e-8
									}
								],
								"quantization": "fp8",
								"contextLength": 128000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"logit_bias",
									"logprobs",
									"top_logprobs",
									"functions",
									"tool_choice",
									"tools"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "nebius",
							"providerModelId": "meta-llama/Meta-Llama-3.1-8B-Instruct-fast",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-8,
									"output": 9e-8
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"logit_bias",
								"logprobs",
								"top_logprobs",
								"functions",
								"tool_choice",
								"tools"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.03,
							"completion": 0.09
						}
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2024-07-23T00:00:00.000Z",
				"description": "Optimized version of Llama 3.1 8B Instruct with 128K context window, designed for high-speed inference in multilingual chat and dialogue use cases with improved throughput and efficiency.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format",
					"tool_choice",
					"tools",
					"structured_outputs",
					"logit_bias",
					"logprobs",
					"top_logprobs",
					"functions"
				]
			},
			{
				"id": "hermes-2-pro-llama-3-8b",
				"name": "Hermes 2 Pro Llama 3 8B",
				"author": "meta-llama",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "meta-llama",
							"modelConfig": {
								"providerModelId": "nousresearch/hermes-2-pro-llama-3-8b",
								"provider": "novita",
								"author": "meta-llama",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.4e-7,
										"output": 1.4e-7
									}
								],
								"quantization": "fp16",
								"contextLength": 8192,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"max_tokens",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"functions",
									"tools",
									"top_k",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "nousresearch/hermes-2-pro-llama-3-8b",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.4e-7,
									"output": 1.4e-7
								}
							],
							"contextLength": 8192,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"functions",
								"tools",
								"top_k",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.14,
							"completion": 0.14
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-05-27T00:00:00.000Z",
				"description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"functions",
					"tools",
					"top_k",
					"top_p"
				]
			},
			{
				"id": "kimi-k2",
				"name": "Kimi K2 Instruct",
				"author": "moonshotai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"providerModelId": "moonshotai/kimi-k2-instruct",
								"provider": "groq",
								"author": "moonshotai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000001,
										"output": 0.000003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "moonshotai/kimi-k2-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000001,
									"output": 0.000003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1,
							"completion": 3,
							"cacheRead": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000105,
										"output": 0.00000316
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "moonshotai/kimi-k2",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000105,
									"output": 0.00000316
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.0499999999999998,
							"completion": 3.1599999999999997
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters.\nTrained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n\nKey Features\nLarge-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\nMuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\nAgentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "kimi-k2-0905",
				"name": "Kimi K2 Instruct (09/05)",
				"author": "moonshotai",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "novita",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2-0905",
								"pricing": [
									{
										"threshold": 0,
										"input": 6e-7,
										"output": 0.0000025
									}
								],
								"contextLength": 262144,
								"maxCompletionTokens": 262144,
								"supportedParameters": [
									"structured_outputs",
									"functions",
									"tool_choice",
									"tools",
									"response_format",
									"max_tokens",
									"temperature",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "moonshotai/kimi-k2-0905",
							"pricing": [
								{
									"threshold": 0,
									"input": 6e-7,
									"output": 0.0000025
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 262144,
							"ptbEnabled": true,
							"supportedParameters": [
								"structured_outputs",
								"functions",
								"tool_choice",
								"tools",
								"response_format",
								"max_tokens",
								"temperature",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.6,
							"completion": 2.5
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"providerModelId": "moonshotai/kimi-k2-instruct-0905",
								"provider": "groq",
								"author": "moonshotai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.000001,
										"output": 0.000003,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0,
										"cacheMultipliers": {
											"cachedInput": 0.5
										}
									}
								],
								"contextLength": 262144,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"frequency_penalty",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "moonshotai/kimi-k2-instruct-0905",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.000001,
									"output": 0.000003,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0,
									"cacheMultipliers": {
										"cachedInput": 0.5
									}
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1,
							"completion": 3,
							"cacheRead": 0.5
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "openrouter",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2-0905",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000142,
										"output": 0.00000528
									}
								],
								"contextLength": 262144,
								"maxCompletionTokens": 262144,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "moonshotai/kimi-k2-0905",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000142,
									"output": 0.00000528
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 262144,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 1.42,
							"completion": 5.28
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-09-05T00:00:00.000Z",
				"description": "Enhanced version of Kimi K2 with doubled context window (256k tokens) and significantly improved coding capabilities, especially for frontend development. Features superior performance on public benchmarks (69.2% on SWE-bench Verified) and more polished code generation for web and 3D scenarios.\n\nKey Improvements\n2x Context Window: Increased from 128k to 256k tokens for better long-horizon task support.\nEnhanced Coding: Specialized training for frontend development, UI code generation, and tool calling.\nAgentic Capabilities: Improved reliability for code generation rivaling frontier closed models.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"functions"
				]
			},
			{
				"id": "kimi-k2-instruct",
				"name": "Kimi K2 Instruct",
				"author": "moonshotai",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "moonshotai",
							"modelConfig": {
								"provider": "novita",
								"author": "moonshotai",
								"providerModelId": "moonshotai/kimi-k2-instruct",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.7e-7,
										"output": 0.0000023
									}
								],
								"quantization": "fp8",
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p",
									"functions",
									"structured_outputs"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "moonshotai/kimi-k2-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.7e-7,
									"output": 0.0000023
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p",
								"functions",
								"structured_outputs"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.5700000000000001,
							"completion": 2.3
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2025-09-28T00:00:00.000Z",
				"description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. Specifically designed for tool use, reasoning, and autonomous problem-solving.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p",
					"functions",
					"structured_outputs"
				]
			},
			{
				"id": "qwen2.5-coder-7b-fast",
				"name": "Qwen2.5 Coder 7B fast",
				"author": "alibaba",
				"contextLength": 32000,
				"endpoints": [
					{
						"provider": "nebius",
						"providerSlug": "nebius",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen2.5-Coder-7B-fast",
								"provider": "nebius",
								"author": "alibaba",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-8,
										"output": 9e-8
									}
								],
								"contextLength": 32000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"logit_bias",
									"logprobs",
									"top_logprobs"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "nebius",
							"providerModelId": "Qwen/Qwen2.5-Coder-7B-fast",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-8,
									"output": 9e-8
								}
							],
							"contextLength": 32000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"logit_bias",
								"logprobs",
								"top_logprobs"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.03,
							"completion": 0.09
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-09-15T00:00:00.000Z",
				"description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"structured_outputs",
					"response_format",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"logit_bias",
					"logprobs",
					"top_logprobs"
				]
			},
			{
				"id": "qwen3-32b",
				"name": "Qwen3 32B",
				"author": "alibaba",
				"contextLength": 131072,
				"endpoints": [
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-32B",
								"provider": "groq",
								"author": "alibaba",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.9e-7,
										"output": 5.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 40960,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"response_format",
									"seed",
									"stop",
									"structured_outputs",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "Qwen/Qwen3-32B",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.9e-7,
									"output": 5.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 40960,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"response_format",
								"seed",
								"stop",
								"structured_outputs",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.29,
							"completion": 0.59
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "qwen/qwen3-32b",
								"provider": "openrouter",
								"author": "alibaba",
								"pricing": [
									{
										"threshold": 0,
										"input": 4.22e-7,
										"output": 8.44e-7
									}
								],
								"contextLength": 40960,
								"maxCompletionTokens": 40960,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "qwen/qwen3-32b",
							"pricing": [
								{
									"threshold": 0,
									"input": 4.22e-7,
									"output": 8.44e-7
								}
							],
							"contextLength": 40960,
							"maxCompletionTokens": 40960,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.422,
							"completion": 0.844
						}
					}
				],
				"maxOutput": 40960,
				"trainingDate": "2025-04-28T00:00:00.000Z",
				"description": "Qwen3-32B is a 32.8 billion parameter language model that uniquely supports seamless switching between thinking mode for complex reasoning tasks and non-thinking mode for efficient general dialogue within a single model. The model excels across 100+ languages with enhanced reasoning capabilities, superior human preference alignment, and strong agent-based task performance, supporting up to 131,072 tokens with YaRN extension.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"response_format",
					"seed",
					"stop",
					"structured_outputs",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p"
				]
			},
			{
				"id": "qwen3-30b-a3b",
				"name": "Qwen3 30B A3B",
				"author": "qwen",
				"contextLength": 41000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-30B-A3B",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 8e-8,
										"output": 2.9e-7
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"contextLength": 32768,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"seed",
									"max_tokens",
									"response_format",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"min_p"
								],
								"ptbEnabled": true,
								"quantization": "fp8",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-30B-A3B",
							"pricing": [
								{
									"threshold": 0,
									"input": 8e-8,
									"output": 2.9e-7
								}
							],
							"contextLength": 32768,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"seed",
								"max_tokens",
								"response_format",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"min_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.08,
							"completion": 0.29
						}
					}
				],
				"maxOutput": 41000,
				"trainingDate": "2025-06-01T00:00:00.000Z",
				"description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"seed",
					"max_tokens",
					"response_format",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"min_p"
				]
			},
			{
				"id": "qwen3-coder",
				"name": "Qwen3 Coder 480B A35B Instruct Turbo",
				"author": "qwen",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.9e-7,
										"output": 0.0000012
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"contextLength": 262144,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"quantization": "fp4",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.9e-7,
									"output": 0.0000012
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.29,
							"completion": 1.2
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-07-23T00:00:00.000Z",
				"description": "Qwen3-Coder-480B-A35B-Instruct is the Qwen3's most agentic code model, featuring significant performance on agentic coding, agentic browser-use and other foundational coding tasks, achieving results comparable to Claude Sonnet. This model supports multimodal capabilities including text, images, audio, video, and audio-visual reasoning.",
				"inputModalities": ["text", "image", "audio", "video"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "qwen3-next-80b-a3b-instruct",
				"name": "Qwen3 Next 80B A3B Instruct",
				"author": "qwen",
				"contextLength": 262000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-Next-80B-A3B-Instruct",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 1.4e-7,
										"output": 0.0000014
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"quantization": "bf16",
								"contextLength": 262000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-Next-80B-A3B-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1.4e-7,
									"output": 0.0000014
								}
							],
							"contextLength": 262000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.14,
							"completion": 1.4
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-01-01T00:00:00.000Z",
				"description": "Qwen3-Next-80B-A3B-Instruct is a causal language model that is instruction-optimized for chat and agent applications. It features a Mixture-of-Experts (MoE) architecture that achieves an extremely low activation ratio, drastically reducing FLOPs per token while preserving model capacity. The model supports ultra-long contexts and has a Multi-Token Prediction (MTP) mechanism to boost performance and accelerate inference.",
				"inputModalities": ["text", "image", "video"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "qwen3-235b-a22b-thinking",
				"name": "Qwen3 235B A22B Thinking",
				"author": "qwen",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-235B-A22B-Thinking-2507",
								"provider": "deepinfra",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000029
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"quantization": "fp8",
								"contextLength": 262000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "Qwen/Qwen3-235B-A22B-Thinking-2507",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000029
								}
							],
							"contextLength": 262000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 2.9000000000000004
						}
					},
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "qwen/qwen3-235b-a22b-thinking-2507",
								"provider": "novita",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.000003
									}
								],
								"quantization": "fp8",
								"contextLength": 131072,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format",
									"functions",
									"structured_outputs",
									"reasoning"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "qwen/qwen3-235b-a22b-thinking-2507",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.000003
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format",
								"functions",
								"structured_outputs",
								"reasoning"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 3
						}
					}
				],
				"maxOutput": 81920,
				"trainingDate": "2025-07-25T00:00:00.000Z",
				"description": "Qwen3-235B-A22B-Thinking-2507 is the Qwen3's new model with scaling the thinking capability of Qwen3-235B-A22B, improving both the quality and depth of reasoning.",
				"inputModalities": ["text", "image", "video"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format",
					"functions",
					"structured_outputs",
					"reasoning"
				]
			},
			{
				"id": "qwen3-vl-235b-a22b-instruct",
				"name": "Qwen3 VL 235B A22B Instruct",
				"author": "alibaba",
				"contextLength": 256000,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "alibaba",
							"modelConfig": {
								"providerModelId": "qwen/qwen3-vl-235b-a22b-instruct",
								"provider": "novita",
								"author": "alibaba",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000015
									}
								],
								"quantization": "bf16",
								"contextLength": 131072,
								"maxCompletionTokens": 32768,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"structured_outputs",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "qwen/qwen3-vl-235b-a22b-instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000015
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 32768,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"structured_outputs",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 1.5
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-09-23T00:00:00.000Z",
				"description": "Qwen3 VL 235B A22B Instruct is a powerful, open-weight multimodal model from Alibaba Cloud that excels at both language and vision tasks. It integrates strong text generation with advanced visual understanding of images and video, enabling applications like visual question answering, document parsing, chart extraction, and multilingual OCR. Key features include robust perception, spatial and long-form video comprehension, and the ability to follow complex instructions in multi-turn dialogues. This model also supports agentic interactions and tool use, including visual coding and GUI automation. ",
				"inputModalities": ["text", "image", "video"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"structured_outputs",
					"response_format",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"min_p",
					"repetition_penalty",
					"logit_bias"
				]
			},
			{
				"id": "qwen3-coder-30b-a3b-instruct",
				"name": "Qwen3 Coder 30B A3B Instruct",
				"author": "alibaba",
				"contextLength": 262144,
				"endpoints": [
					{
						"provider": "nebius",
						"providerSlug": "nebius",
						"endpoint": {
							"author": "qwen",
							"modelConfig": {
								"providerModelId": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
								"provider": "nebius",
								"author": "qwen",
								"pricing": [
									{
										"threshold": 0,
										"input": 1e-7,
										"output": 3e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 262144,
								"maxCompletionTokens": 262144,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"logit_bias",
									"logprobs",
									"top_logprobs"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "nebius",
							"providerModelId": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
							"pricing": [
								{
									"threshold": 0,
									"input": 1e-7,
									"output": 3e-7
								}
							],
							"contextLength": 262144,
							"maxCompletionTokens": 262144,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"logit_bias",
								"logprobs",
								"top_logprobs"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.09999999999999999,
							"completion": 0.3
						}
					}
				],
				"maxOutput": 262144,
				"trainingDate": "2025-07-31T00:00:00.000Z",
				"description": "This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements: (a) Significant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks. (b) Long-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding. (c) Agentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"logit_bias",
					"logprobs",
					"top_logprobs"
				]
			},
			{
				"id": "deepseek-r1-distill-llama-70b",
				"name": "DeepSeek R1 Distill Llama 70B",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "deepseek",
								"providerModelId": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
								"pricing": [
									{
										"threshold": 0,
										"input": 2e-7,
										"output": 6e-7
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 16384,
								"quantization": "fp8",
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
							"pricing": [
								{
									"threshold": 0,
									"input": 2e-7,
									"output": 6e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.19999999999999998,
							"completion": 0.6
						}
					},
					{
						"provider": "groq",
						"providerSlug": "groq",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"providerModelId": "deepseek-r1-distill-llama-70b",
								"provider": "groq",
								"author": "deepseek",
								"pricing": [
									{
										"threshold": 0,
										"input": 7.5e-7,
										"output": 9.9e-7,
										"request": 0,
										"image": 0,
										"audio": 0,
										"web_search": 0
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"include_reasoning",
									"logit_bias",
									"logprobs",
									"max_tokens",
									"min_p",
									"presence_penalty",
									"reasoning",
									"repetition_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_k",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "groq",
							"providerModelId": "deepseek-r1-distill-llama-70b",
							"pricing": [
								{
									"threshold": 0,
									"input": 7.5e-7,
									"output": 9.9e-7,
									"request": 0,
									"image": 0,
									"audio": 0,
									"web_search": 0
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"include_reasoning",
								"logit_bias",
								"logprobs",
								"max_tokens",
								"min_p",
								"presence_penalty",
								"reasoning",
								"repetition_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_k",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.75,
							"completion": 0.9900000000000001
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-r1-distill-llama-70b",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000211,
										"output": 0.00000211
									}
								],
								"contextLength": 131072,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"seed",
									"stop",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-r1-distill-llama-70b",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000211,
									"output": 0.00000211
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"seed",
								"stop",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 2.1100000000000003,
							"completion": 2.1100000000000003
						}
					}
				],
				"maxOutput": 4096,
				"trainingDate": "2025-01-20T00:00:00.000Z",
				"description": "DeepSeek-R1-Distill-Llama-70B is a 70-billion parameter model created by distilling the reasoning capabilities of DeepSeek's flagship R1 model (671B parameters) into Meta's Llama-3.3-70B-Instruct base. It achieves exceptional performance on mathematical reasoning and coding benchmarks (94.5% on MATH-500, 1633 CodeForces rating), rivaling OpenAI's o1-mini while being fully open-source under MIT license. The model demonstrates that advanced reasoning patterns from larger models can be effectively transferred to smaller, more deployable architectures through knowledge distillation.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"include_reasoning",
					"logit_bias",
					"logprobs",
					"max_tokens",
					"min_p",
					"presence_penalty",
					"reasoning",
					"repetition_penalty",
					"seed",
					"stop",
					"temperature",
					"tool_choice",
					"tools",
					"top_k",
					"top_logprobs",
					"top_p",
					"response_format"
				]
			},
			{
				"id": "deepseek-v3",
				"name": "DeepSeek V3",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "deepseek",
								"providerModelId": "deepseek-ai/DeepSeek-V3.1",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.7e-7,
										"output": 0.000001
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"quantization": "fp4",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "deepseek-ai/DeepSeek-V3.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.7e-7,
									"output": 0.000001
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.27,
							"completion": 1
						}
					},
					{
						"provider": "deepseek",
						"providerSlug": "deepseek",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepseek",
								"author": "deepseek",
								"providerModelId": "deepseek-chat",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.6e-7,
										"output": 0.00000168,
										"cacheMultipliers": {
											"cachedInput": 0.125
										}
									}
								],
								"contextLength": 128000,
								"maxCompletionTokens": 8192,
								"supportedParameters": [
									"frequency_penalty",
									"function_call",
									"functions",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepseek",
							"providerModelId": "deepseek-chat",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.6e-7,
									"output": 0.00000168,
									"cacheMultipliers": {
										"cachedInput": 0.125
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 8192,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"function_call",
								"functions",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.56,
							"completion": 1.68,
							"cacheRead": 0.07
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-chat-v3.1",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000316,
										"output": 0.00000475
									}
								],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"function_call",
									"functions",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"tool_choice",
									"tools",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-chat-v3.1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000316,
									"output": 0.00000475
								}
							],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"function_call",
								"functions",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"tool_choice",
								"tools",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3.1599999999999997,
							"completion": 4.75
						}
					}
				],
				"maxOutput": 8192,
				"trainingDate": "2024-12-26T00:00:00.000Z",
				"description": "DeepSeek-V3.1 (deepseek-chat) is a powerful generalist model with 671B parameters, offering exceptional performance at an economical price. It achieves strong results on mathematical reasoning, coding, and general language tasks. The model supports 128K context length with a default output of 4K tokens (max 8K) and features advanced capabilities like function calling and JSON output.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"function_call",
					"functions",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"stream",
					"temperature",
					"tool_choice",
					"tools",
					"top_logprobs",
					"top_p",
					"repetition_penalty",
					"top_k",
					"min_p"
				]
			},
			{
				"id": "deepseek-v3.1-terminus",
				"name": "DeepSeek V3.1 Terminus",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "deepseek",
								"providerModelId": "deepseek-ai/DeepSeek-V3.1-Terminus",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.7e-7,
										"output": 0.000001,
										"cacheMultipliers": {
											"cachedInput": 0.8
										}
									}
								],
								"quantization": "fp4",
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "deepseek-ai/DeepSeek-V3.1-Terminus",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.7e-7,
									"output": 0.000001,
									"cacheMultipliers": {
										"cachedInput": 0.8
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.27,
							"completion": 1,
							"cacheRead": 0.21600000000000003
						}
					},
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "novita",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-v3.1-terminus",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.7e-7,
										"output": 0.000001
									}
								],
								"quantization": "fp8",
								"contextLength": 98304,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"functions",
									"structured_outputs",
									"reasoning",
									"tool_choice",
									"tools",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "deepseek/deepseek-v3.1-terminus",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.7e-7,
									"output": 0.000001
								}
							],
							"contextLength": 98304,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"functions",
								"structured_outputs",
								"reasoning",
								"tool_choice",
								"tools",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.27,
							"completion": 1
						}
					}
				],
				"maxOutput": 16384,
				"trainingDate": "2025-09-22T00:00:00.000Z",
				"description": "DeepSeek-V3.1 Terminus is an update to DeepSeek V3.1 that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process. Users can control the reasoning behaviour with the reasoning enabled boolean. The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format",
					"functions",
					"structured_outputs",
					"reasoning",
					"tool_choice",
					"tools",
					"logit_bias"
				]
			},
			{
				"id": "deepseek-v3.2",
				"name": "DeepSeek V3.2",
				"author": "deepseek",
				"contextLength": 163840,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "novita",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-v3.2-exp",
								"pricing": [
									{
										"threshold": 0,
										"input": 2.7e-7,
										"output": 4.1e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"tools",
									"tool_choice",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "deepseek/deepseek-v3.2-exp",
							"pricing": [
								{
									"threshold": 0,
									"input": 2.7e-7,
									"output": 4.1e-7
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"tools",
								"tool_choice",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.27,
							"completion": 0.41
						}
					}
				],
				"maxOutput": 65536,
				"trainingDate": "2025-09-22T00:00:00.000Z",
				"description": "DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality. This delivers substantial computational efficiency improvements without compromising accuracy. Comprehensive benchmarks confirm V3.2-Exp matches V3.1-Terminus performance, proving efficiency gains don't sacrifice capability. As both a powerful tool and research platform, it establishes new paradigms for efficient long-context AI processing.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"tools",
					"tool_choice",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"min_p",
					"repetition_penalty",
					"logit_bias"
				]
			},
			{
				"id": "deepseek-reasoner",
				"name": "DeepSeek Reasoner",
				"author": "deepseek",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepseek",
						"providerSlug": "deepseek",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepseek",
								"author": "deepseek",
								"providerModelId": "deepseek-reasoner",
								"pricing": [
									{
										"threshold": 0,
										"input": 5.6e-7,
										"output": 0.00000168,
										"cacheMultipliers": {
											"cachedInput": 0.125
										}
									}
								],
								"quantization": "fp4",
								"contextLength": 128000,
								"maxCompletionTokens": 64000,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepseek",
							"providerModelId": "deepseek-reasoner",
							"pricing": [
								{
									"threshold": 0,
									"input": 5.6e-7,
									"output": 0.00000168,
									"cacheMultipliers": {
										"cachedInput": 0.125
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 64000,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"top_logprobs",
								"top_p"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.56,
							"completion": 1.68,
							"cacheRead": 0.07
						}
					},
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "deepinfra",
								"author": "deepseek",
								"providerModelId": "deepseek-ai/DeepSeek-R1-0528",
								"pricing": [
									{
										"threshold": 0,
										"input": 5e-7,
										"output": 0.00000215,
										"cacheMultipliers": {
											"cachedInput": 0.8
										}
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "deepseek-ai/DeepSeek-R1-0528",
							"pricing": [
								{
									"threshold": 0,
									"input": 5e-7,
									"output": 0.00000215,
									"cacheMultipliers": {
										"cachedInput": 0.8
									}
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.5,
							"completion": 2.1500000000000004,
							"cacheRead": 0.39999999999999997
						}
					},
					{
						"provider": "openrouter",
						"providerSlug": "openrouter",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "openrouter",
								"author": "deepseek",
								"providerModelId": "deepseek/deepseek-r1",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.00000316,
										"output": 0.00000844
									}
								],
								"contextLength": 163840,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"frequency_penalty",
									"logprobs",
									"max_tokens",
									"presence_penalty",
									"response_format",
									"seed",
									"stop",
									"stream",
									"temperature",
									"top_logprobs",
									"top_p"
								],
								"ptbEnabled": true,
								"priority": 3,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "openrouter",
							"providerModelId": "deepseek/deepseek-r1",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.00000316,
									"output": 0.00000844
								}
							],
							"contextLength": 163840,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"frequency_penalty",
								"logprobs",
								"max_tokens",
								"presence_penalty",
								"response_format",
								"seed",
								"stop",
								"stream",
								"temperature",
								"top_logprobs",
								"top_p"
							],
							"priority": 3
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 3.1599999999999997,
							"completion": 8.440000000000001
						}
					}
				],
				"maxOutput": 64000,
				"trainingDate": "2025-01-20T00:00:00.000Z",
				"description": "DeepSeek-Reasoner (DeepSeek-V3.1 Thinking Mode) is designed for advanced reasoning, mathematical problem-solving, and complex coding tasks. It uses chain-of-thought reasoning to break down complex problems and achieve superior performance on reasoning benchmarks. Supports 128K context with a default output of 32K tokens (max 64K) for extensive reasoning chains.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"frequency_penalty",
					"logprobs",
					"max_tokens",
					"presence_penalty",
					"response_format",
					"seed",
					"stop",
					"stream",
					"temperature",
					"top_logprobs",
					"top_p",
					"repetition_penalty",
					"top_k",
					"min_p"
				]
			},
			{
				"id": "deepseek-tng-r1t2-chimera",
				"name": "DeepSeek TNG R1T2 Chimera",
				"author": "deepseek",
				"contextLength": 130000,
				"endpoints": [
					{
						"provider": "chutes",
						"providerSlug": "chutes",
						"endpoint": {
							"author": "deepseek",
							"modelConfig": {
								"provider": "chutes",
								"author": "deepseek",
								"providerModelId": "tngtech/DeepSeek-TNG-R1T2-Chimera",
								"pricing": [
									{
										"threshold": 0,
										"input": 3e-7,
										"output": 0.0000012
									}
								],
								"contextLength": 130000,
								"maxCompletionTokens": 163840,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"logprobs",
									"logit_bias",
									"top_logprobs",
									"functions",
									"tools"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "chutes",
							"providerModelId": "tngtech/DeepSeek-TNG-R1T2-Chimera",
							"pricing": [
								{
									"threshold": 0,
									"input": 3e-7,
									"output": 0.0000012
								}
							],
							"contextLength": 130000,
							"maxCompletionTokens": 163840,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"logprobs",
								"logit_bias",
								"top_logprobs",
								"functions",
								"tools"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.3,
							"completion": 1.2
						}
					}
				],
				"maxOutput": 163840,
				"trainingDate": "2025-07-02T00:00:00.000Z",
				"description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AIs R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2 faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"logprobs",
					"logit_bias",
					"top_logprobs",
					"functions",
					"tools"
				]
			},
			{
				"id": "mistral-nemo",
				"name": "Mistral Nemo",
				"author": "mistralai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "mistralai",
							"modelConfig": {
								"providerModelId": "mistralai/Mistral-Nemo-Instruct-2407",
								"provider": "deepinfra",
								"author": "mistralai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.02,
										"output": 0.04
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"quantization": "fp8",
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "mistralai/Mistral-Nemo-Instruct-2407",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.02,
									"output": 0.04
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 20000,
							"completion": 40000
						}
					}
				],
				"maxOutput": 16400,
				"trainingDate": "2024-07-18T00:00:00.000Z",
				"description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "mistral-small",
				"name": "Mistral Small",
				"author": "mistralai",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "deepinfra",
						"providerSlug": "deepinfra",
						"endpoint": {
							"author": "mistralai",
							"modelConfig": {
								"providerModelId": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
								"provider": "deepinfra",
								"author": "mistralai",
								"pricing": [
									{
										"threshold": 0,
										"input": 0.05,
										"output": 0.1
									}
								],
								"rateLimits": {
									"rpm": 12000,
									"tpm": 60000000,
									"tpd": 6000000000
								},
								"contextLength": 128000,
								"maxCompletionTokens": 16384,
								"supportedParameters": [
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"repetition_penalty",
									"top_k",
									"seed",
									"min_p",
									"response_format"
								],
								"ptbEnabled": true,
								"quantization": "fp8",
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "deepinfra",
							"providerModelId": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
							"pricing": [
								{
									"threshold": 0,
									"input": 0.05,
									"output": 0.1
								}
							],
							"contextLength": 128000,
							"maxCompletionTokens": 16384,
							"ptbEnabled": true,
							"supportedParameters": [
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"repetition_penalty",
								"top_k",
								"seed",
								"min_p",
								"response_format"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 50000,
							"completion": 100000
						}
					}
				],
				"maxOutput": 128000,
				"trainingDate": "2024-02-26T00:00:00.000Z",
				"description": "Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks. It supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).",
				"inputModalities": ["text", "image"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"repetition_penalty",
					"top_k",
					"seed",
					"min_p",
					"response_format"
				]
			},
			{
				"id": "glm-4.6",
				"name": "Zai GLM-4.6",
				"author": "zai",
				"contextLength": 204800,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "zai",
							"modelConfig": {
								"providerModelId": "zai-org/glm-4.6",
								"provider": "novita",
								"author": "zai",
								"pricing": [
									{
										"threshold": 0,
										"input": 6e-7,
										"output": 0.0000022
									}
								],
								"quantization": "bf16",
								"contextLength": 204800,
								"maxCompletionTokens": 131072,
								"supportedParameters": [
									"functions",
									"structured_outputs",
									"reasoning",
									"tool_choice",
									"tools",
									"response_format",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "zai-org/glm-4.6",
							"pricing": [
								{
									"threshold": 0,
									"input": 6e-7,
									"output": 0.0000022
								}
							],
							"contextLength": 204800,
							"maxCompletionTokens": 131072,
							"ptbEnabled": true,
							"supportedParameters": [
								"functions",
								"structured_outputs",
								"reasoning",
								"tool_choice",
								"tools",
								"response_format",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.6,
							"completion": 2.2
						}
					}
				],
				"maxOutput": 131072,
				"trainingDate": "2024-07-18T00:00:00.000Z",
				"description": "As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"functions",
					"structured_outputs",
					"reasoning",
					"tool_choice",
					"tools",
					"response_format",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"min_p",
					"repetition_penalty",
					"logit_bias"
				]
			},
			{
				"id": "ernie-4.5-21b-a3b-thinking",
				"name": "Baidu Ernie 4.5 21B A3B Thinking",
				"author": "baidu",
				"contextLength": 128000,
				"endpoints": [
					{
						"provider": "novita",
						"providerSlug": "novita",
						"endpoint": {
							"author": "baidu",
							"modelConfig": {
								"provider": "novita",
								"author": "baidu",
								"providerModelId": "baidu/ernie-4.5-21B-A3B-Thinking",
								"pricing": [
									{
										"threshold": 0,
										"input": 7e-8,
										"output": 2.8e-7
									}
								],
								"quantization": "fp8",
								"contextLength": 131072,
								"maxCompletionTokens": 65536,
								"supportedParameters": [
									"reasoning",
									"max_tokens",
									"temperature",
									"top_p",
									"stop",
									"frequency_penalty",
									"presence_penalty",
									"seed",
									"top_k",
									"min_p",
									"repetition_penalty",
									"logit_bias"
								],
								"ptbEnabled": true,
								"endpointConfigs": {
									"*": {}
								}
							},
							"userConfig": {
								"region": "*",
								"location": "*"
							},
							"provider": "novita",
							"providerModelId": "baidu/ernie-4.5-21B-A3B-Thinking",
							"pricing": [
								{
									"threshold": 0,
									"input": 7e-8,
									"output": 2.8e-7
								}
							],
							"contextLength": 131072,
							"maxCompletionTokens": 65536,
							"ptbEnabled": true,
							"supportedParameters": [
								"reasoning",
								"max_tokens",
								"temperature",
								"top_p",
								"stop",
								"frequency_penalty",
								"presence_penalty",
								"seed",
								"top_k",
								"min_p",
								"repetition_penalty",
								"logit_bias"
							]
						},
						"supportsPtb": true,
						"pricing": {
							"prompt": 0.07,
							"completion": 0.28
						}
					}
				],
				"maxOutput": 8000,
				"trainingDate": "2025-03-16T00:00:00.000Z",
				"description": "ERNIE-4.5-21B-A3B-Thinking is a text-based Mixture of Experts (MoE) post-training model featuring 21B total parameters with 3B active parameters per token. It delivers enhanced performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. The model offers efficient tool utilization capabilities and supports up to 128K tokens for long-context understanding.",
				"inputModalities": ["text"],
				"outputModalities": ["text"],
				"supportedParameters": [
					"reasoning",
					"max_tokens",
					"temperature",
					"top_p",
					"stop",
					"frequency_penalty",
					"presence_penalty",
					"seed",
					"top_k",
					"min_p",
					"repetition_penalty",
					"logit_bias"
				]
			}
		],
		"total": 65,
		"filters": {
			"providers": [
				{
					"name": "anthropic",
					"displayName": "Anthropic"
				},
				{
					"name": "azure",
					"displayName": "Azure OpenAI"
				},
				{
					"name": "bedrock",
					"displayName": "AWS Bedrock"
				},
				{
					"name": "chutes",
					"displayName": "Chutes"
				},
				{
					"name": "deepinfra",
					"displayName": "DeepInfra"
				},
				{
					"name": "deepseek",
					"displayName": "DeepSeek"
				},
				{
					"name": "google-ai-studio",
					"displayName": "Google AI Studio"
				},
				{
					"name": "groq",
					"displayName": "Groq"
				},
				{
					"name": "nebius",
					"displayName": "Nebius"
				},
				{
					"name": "novita",
					"displayName": "Novita"
				},
				{
					"name": "openai",
					"displayName": "OpenAI"
				},
				{
					"name": "openrouter",
					"displayName": "OpenRouter"
				},
				{
					"name": "vertex",
					"displayName": "Vertex AI"
				},
				{
					"name": "xai",
					"displayName": "xAI"
				}
			],
			"authors": [
				"alibaba",
				"anthropic",
				"baidu",
				"deepseek",
				"google",
				"meta-llama",
				"mistralai",
				"moonshotai",
				"openai",
				"qwen",
				"xai",
				"zai"
			],
			"capabilities": ["audio", "caching", "image", "web_search"]
		}
	},
	"error": null
}
